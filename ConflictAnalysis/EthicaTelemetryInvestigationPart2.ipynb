{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T16:21:34.871428Z",
     "start_time": "2020-08-11T16:20:57.008992Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "# tell Seaborn that we're producing a document and not a slideshow or poster\n",
    "sns.set()\n",
    "sns.set_context('paper')\n",
    "\n",
    "# expects to find connection credentials in local runtime environment\n",
    "db_host=os.environ['SQL_LOCAL_SERVER']\n",
    "db_host_port=int(os.environ['SQL_LOCAL_PORT'])\n",
    "db_user=os.environ['SQL_USER']\n",
    "db_name=os.environ['SQL_DB']\n",
    "db_schema=os.environ['SQL_SCHEMA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Following a team discussion about the previous investigation, we decided to explore three further questions.\n",
    "\n",
    "1. Assess duty cycle counts and estimate losses if we remove the redundant records\n",
    "2. Assess synchronization between observations in GPS vs XL\n",
    "3. Assess synchronization between duty cycles in GPS vs XL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Lost DCs\n",
    "The first step is to generate the list of viable DCs for each user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tables of unique observations\n",
    "\n",
    "A unique observation is a specific user_id and timestamp. The raw data contains many duplicated and/or conflicted sensor readings, but when we ignore the actual sensor components, we can extract the set of unique observation ids for which we have data. We are also interested in comparing those unfiltered observation ids with the smaller set of records for which there are no duplicates or conflicts of any kind. We call these the singleton observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T16:40:46.896004Z",
     "start_time": "2020-08-11T16:40:29.535628Z"
    },
    "code_folding": [
     3,
     16
    ],
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Some Unique ObservationIDs from GPS\n",
      "  numrecs    user_id  record_time\n",
      "---------  ---------  --------------------------------\n",
      "        9       1030  2019-01-21 23:41:42.419000-08:00\n",
      "        1       1030  2019-01-21 23:41:50.238000-08:00\n",
      "        1       1030  2019-01-21 23:41:51.242000-08:00\n",
      "        1       1030  2019-01-21 23:41:52.240000-08:00\n",
      "        1       1030  2019-01-21 23:41:53.239000-08:00\n",
      "        1       1030  2019-01-21 23:41:54.238000-08:00\n",
      "        1       1030  2019-01-21 23:41:55.231000-08:00\n",
      "        1       1030  2019-01-21 23:41:56.218000-08:00\n",
      "        1       1030  2019-01-21 23:41:57.227000-08:00\n",
      "        1       1030  2019-01-21 23:41:58.237000-08:00\n",
      "\n",
      "\n",
      "\n",
      "Figure: Some Unique Singleton ObservationIDs from GPS\n",
      "  numrecs    user_id  record_time\n",
      "---------  ---------  --------------------------------\n",
      "        1       1030  2019-01-21 23:41:50.238000-08:00\n",
      "        1       1030  2019-01-21 23:41:51.242000-08:00\n",
      "        1       1030  2019-01-21 23:41:52.240000-08:00\n",
      "        1       1030  2019-01-21 23:41:53.239000-08:00\n",
      "        1       1030  2019-01-21 23:41:54.238000-08:00\n",
      "        1       1030  2019-01-21 23:41:55.231000-08:00\n",
      "        1       1030  2019-01-21 23:41:56.218000-08:00\n",
      "        1       1030  2019-01-21 23:41:57.227000-08:00\n",
      "        1       1030  2019-01-21 23:41:58.237000-08:00\n",
      "        1       1030  2019-01-21 23:41:59.249000-08:00\n",
      "\n",
      "\n",
      "\n",
      "Figure: Some Unique Unconflicted ObservationIDs from GPS\n",
      "  numrecs    user_id  record_time                           lat       lon\n",
      "---------  ---------  --------------------------------  -------  --------\n",
      "        1       1030  2019-01-21 23:41:42.419000-08:00  52.1159  -106.643\n",
      "        1       1030  2019-01-21 23:41:50.238000-08:00  52.1159  -106.643\n",
      "        1       1030  2019-01-21 23:41:51.242000-08:00  52.1159  -106.643\n",
      "        1       1030  2019-01-21 23:41:52.240000-08:00  52.1159  -106.643\n",
      "        1       1030  2019-01-21 23:41:53.239000-08:00  52.1159  -106.643\n",
      "        1       1030  2019-01-21 23:41:54.238000-08:00  52.1158  -106.643\n",
      "        1       1030  2019-01-21 23:41:55.231000-08:00  52.1157  -106.643\n",
      "        1       1030  2019-01-21 23:41:56.218000-08:00  52.1163  -106.642\n",
      "        1       1030  2019-01-21 23:41:57.227000-08:00  52.1161  -106.642\n",
      "        1       1030  2019-01-21 23:41:58.237000-08:00  52.1161  -106.643\n"
     ]
    }
   ],
   "source": [
    "# Collapse GPS table into a tmp table with unique observationids\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS level_0.tmpuniqgpsobs AS\n",
    "            SELECT numrecs, user_id, record_time\n",
    "            FROM (\n",
    "                SELECT count(1) as numrecs, user_id, record_time FROM level_0.tmpsskgps\n",
    "                GROUP BY user_id, record_time\n",
    "                ) grouped\n",
    "                \"\"\"    \n",
    "    cur.execute(sql)\n",
    "    # now add an index to the new table (can't be done in one step)\n",
    "    cur.execute(\"CREATE INDEX ON level_0.tmpuniqgpsobs (user_id, record_time)\")\n",
    "    \n",
    "    # create a singleton table by dropping any records with any kind of timestamp collision\n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS level_0.tmpuniqgpssingletonobs AS\n",
    "            SELECT numrecs, user_id, record_time\n",
    "            FROM level_0.tmpuniqgpsobs\n",
    "            WHERE numrecs = 1\n",
    "            \"\"\"    \n",
    "    cur.execute(sql)\n",
    "    # and add an index\n",
    "    cur.execute(\"CREATE INDEX ON level_0.tmpuniqgpssingletonobs (user_id, record_time)\")\n",
    "       \n",
    "    # create a conflictless table by dropping records with multiple sensor values\n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS level_0.tmpuniqgpsunconflictedobs AS\n",
    "            SELECT 1 as numrecs, user_id, record_time, minlat as lat, minlon as lon\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       user_id, \n",
    "                       record_time, \n",
    "                       min(lat) as minlat, \n",
    "                       max(lat) as maxlat,\n",
    "                       min(lon) as minlon,\n",
    "                       max(lon) as maxlon\n",
    "                FROM level_0.tmpsskgps\n",
    "                GROUP BY user_id, record_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1 -- there are no duplicates at all\n",
    "                OR (minlat = maxlat AND minlon = maxlon) -- the duplicates are identical\n",
    "            \"\"\"    \n",
    "    cur.execute(sql)\n",
    "    # and add an index\n",
    "    cur.execute(\"CREATE INDEX ON level_0.tmpuniqgpsunconflictedobs (user_id, record_time)\")\n",
    "\n",
    "    \n",
    "    cur.execute(\"SELECT numrecs, user_id, record_time FROM level_0.tmpuniqgpsobs LIMIT 10;\")\n",
    "    rows = cur.fetchall()\n",
    "    print(\"Figure: Some Unique ObservationIDs from GPS\")\n",
    "    print(tabulate(rows, headers='keys'))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    cur.execute(\"SELECT numrecs, user_id, record_time FROM level_0.tmpuniqgpssingletonobs LIMIT 10;\")\n",
    "    rows = cur.fetchall()\n",
    "    print(\"Figure: Some Unique Singleton ObservationIDs from GPS\")\n",
    "    print(tabulate(rows, headers='keys'))\n",
    "        \n",
    "    print(\"\\n\\n\")\n",
    "    cur.execute(\"SELECT numrecs, user_id, record_time, lat, lon FROM level_0.tmpuniqgpsunconflictedobs LIMIT 10;\")\n",
    "    rows = cur.fetchall()\n",
    "    print(\"Figure: Some Unique Unconflicted ObservationIDs from GPS\")\n",
    "    print(tabulate(rows, headers='keys'))\n",
    "\n",
    "    \n",
    "    # Count records in relevant tables\n",
    "    cur.execute(\"SELECT count(1) from level_0.tmpsskgps\")\n",
    "    row = cur.fetchone()\n",
    "    num_raw_gps_recs = row['count']\n",
    "\n",
    "    cur.execute(\"SELECT count(1) from level_0.tmpuniqgpsobs\")\n",
    "    row = cur.fetchone()\n",
    "    num_unique_gps_obs = row['count']\n",
    "        \n",
    "    cur.execute(\"SELECT count(1) from level_0.tmpuniqgpssingletonobs\")\n",
    "    row = cur.fetchone()\n",
    "    num_unique_gps_singleton_obs = row['count']\n",
    "      \n",
    "    cur.execute(\"SELECT count(1) from level_0.tmpuniqgpsunconflictedobs\")\n",
    "    row = cur.fetchone()\n",
    "    num_unique_gps_unconflicted_obs = row['count']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T16:41:30.810277Z",
     "start_time": "2020-08-11T16:41:30.801224Z"
    },
    "code_folding": [],
    "scrolled": true,
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Figure: GPS Record Counts\n",
      "----------------------------------------  ----------\n",
      "                     Num Raw GPS Records  28,897,052\n",
      "             Num Unique GPS Observations   8,131,279\n",
      "   Num Unique GPS Singleton Observations   7,729,649\n",
      "Num Unique GPS Unconflicted Observations   7,791,943\n",
      "----------------------------------------  ----------\n"
     ]
    }
   ],
   "source": [
    "    print(\"\\n\\nFigure: GPS Record Counts\")\n",
    "    data = [[f\"Num Raw GPS Records\",f\"{num_raw_gps_recs:,}\"],\n",
    "            [f\"Num Unique GPS Observations\",f\"{num_unique_gps_obs:,}\"],\n",
    "            [f\"Num Unique GPS Singleton Observations\",f\"{num_unique_gps_singleton_obs:,}\"],\n",
    "            [f\"Num Unique GPS Unconflicted Observations\",f\"{num_unique_gps_unconflicted_obs:,}\"]]\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:45:29.249387Z",
     "start_time": "2020-08-11T16:46:38.534002Z"
    },
    "scrolled": true,
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Figure: Some Unique ObservationIDs from XL\n",
      "  numrecs    user_id  record_time\n",
      "---------  ---------  --------------------------------\n",
      "        1       1030  2019-01-21 23:39:27.991000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.053000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.114000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.174000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.238000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.299000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.360000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.421000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.482000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.545000-08:00\n",
      "\n",
      "\n",
      "\n",
      "Figure: Some Unique Singleton ObservationIDs from XL\n",
      "  numrecs    user_id  record_time\n",
      "---------  ---------  --------------------------------\n",
      "        1       1030  2019-01-21 23:39:27.991000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.053000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.114000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.174000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.238000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.299000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.360000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.421000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.482000-08:00\n",
      "        1       1030  2019-01-21 23:39:28.545000-08:00\n",
      "\n",
      "\n",
      "\n",
      "Figure: Some Unique Unconflicted ObservationIDs from XL\n",
      "  numrecs    user_id  record_time                         x_axis    y_axis    z_axis\n",
      "---------  ---------  --------------------------------  --------  --------  --------\n",
      "        1       1030  2019-01-21 23:39:27.991000-08:00   4.77344   7.80209  -3.37343\n",
      "        1       1030  2019-01-21 23:39:28.053000-08:00   4.78302   7.75421  -3.35905\n",
      "        1       1030  2019-01-21 23:39:28.114000-08:00   4.85005   7.72069  -3.53622\n",
      "        1       1030  2019-01-21 23:39:28.174000-08:00   4.87878   7.69675  -3.50751\n",
      "        1       1030  2019-01-21 23:39:28.238000-08:00   4.80696   7.73027  -3.4261\n",
      "        1       1030  2019-01-21 23:39:28.299000-08:00   4.85484   7.68718  -3.33511\n",
      "        1       1030  2019-01-21 23:39:28.360000-08:00   4.79259   7.78294  -3.60806\n",
      "        1       1030  2019-01-21 23:39:28.421000-08:00   4.85963   7.73985  -3.31596\n",
      "        1       1030  2019-01-21 23:39:28.482000-08:00   4.84047   7.67281  -3.47878\n",
      "        1       1030  2019-01-21 23:39:28.545000-08:00   4.78302   7.78294  -3.50751\n"
     ]
    }
   ],
   "source": [
    "# Collapse Accel table into a tmp table with unique observationids\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS level_0.tmpuniqaccelobs AS\n",
    "        SELECT numrecs, user_id, record_time\n",
    "        FROM (\n",
    "            SELECT count(1) as numrecs, user_id, record_time FROM level_0.tmpsskaccel\n",
    "            GROUP BY user_id, record_time\n",
    "            ) foo\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    # and add an index\n",
    "    cur.execute(\"CREATE INDEX ON level_0.tmpuniqaccelobs (user_id, record_time)\")\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS level_0.tmpuniqaccelsingletonobs AS\n",
    "            SELECT numrecs, user_id, record_time\n",
    "            FROM level_0.tmpuniqaccelobs\n",
    "            WHERE numrecs = 1\n",
    "            \"\"\"    \n",
    "    cur.execute(sql)\n",
    "    # and add an index\n",
    "    cur.execute(\"CREATE INDEX ON level_0.tmpuniqaccelsingletonobs (user_id, record_time)\")\n",
    "\n",
    "    # create a conflictless table by dropping records with multiple sensor values\n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS level_0.tmpuniqaccelunconflictedobs AS\n",
    "            SELECT 1 as numrecs, user_id, record_time, \n",
    "                   minx as x_axis, miny as y_axis, minz as z_axis\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       user_id, \n",
    "                       record_time, \n",
    "                       min(x_axis) as minx, \n",
    "                       max(x_axis) as maxx,\n",
    "                       min(y_axis) as miny,\n",
    "                       max(y_axis) as maxy,\n",
    "                       min(z_axis) as minz,\n",
    "                       max(z_axis) as maxz\n",
    "                FROM level_0.tmpsskaccel\n",
    "                GROUP BY user_id, record_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1 -- there are no duplicates at all\n",
    "                OR (minx = maxx AND miny = maxy AND minz = maxz) -- the duplicates are identical\n",
    "            \"\"\"    \n",
    "    cur.execute(sql)\n",
    "    # and add an index\n",
    "    cur.execute(\"CREATE INDEX ON level_0.tmpuniqaccelunconflictedobs (user_id, record_time)\")\n",
    "        \n",
    "    cur.execute(\"SELECT * FROM level_0.tmpuniqaccelobs LIMIT 10;\")\n",
    "    rows = cur.fetchall()\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Figure: Some Unique ObservationIDs from XL\")\n",
    "    print(tabulate(rows, headers='keys'))\n",
    "    \n",
    "    cur.execute(\"SELECT numrecs, user_id, record_time FROM level_0.tmpuniqaccelsingletonobs LIMIT 10;\")\n",
    "    rows = cur.fetchall()\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Figure: Some Unique Singleton ObservationIDs from XL\")\n",
    "    print(tabulate(rows, headers='keys'))\n",
    "            \n",
    "    print(\"\\n\\n\")\n",
    "    cur.execute(\"SELECT numrecs, user_id, record_time, x_axis, y_axis, z_axis FROM level_0.tmpuniqaccelunconflictedobs LIMIT 10;\")\n",
    "    rows = cur.fetchall()\n",
    "    print(\"Figure: Some Unique Unconflicted ObservationIDs from XL\")\n",
    "    print(tabulate(rows, headers='keys'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:52:33.561329Z",
     "start_time": "2020-08-11T17:47:15.914427Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Accel Record Counts:\n",
      "------------------------------------------  -----------\n",
      "                     Num Raw Accel Records  960,178,307\n",
      "             Num Unique Accel Observations  878,955,386\n",
      "   Num Unique Accel Singleton Observations  832,152,864\n",
      "Num Unique Accel Unconflicted Observations  836,034,501\n",
      "------------------------------------------  -----------\n"
     ]
    }
   ],
   "source": [
    "# Report counts of relevant tables\n",
    "cur.execute(\"SELECT count(1) from level_0.tmpsskaccel\")\n",
    "row = cur.fetchone()\n",
    "num_raw_accel_recs = row['count']\n",
    "\n",
    "\n",
    "cur.execute(\"SELECT count(1) from level_0.tmpuniqaccelobs\")\n",
    "row = cur.fetchone()\n",
    "num_unique_accel_obs = row['count']\n",
    "\n",
    "\n",
    "cur.execute(\"SELECT count(1) from level_0.tmpuniqaccelsingletonobs\")\n",
    "row = cur.fetchone()\n",
    "num_unique_accel_singleton_obs = row['count']\n",
    "\n",
    "\n",
    "cur.execute(\"SELECT count(1) from level_0.tmpuniqaccelunconflictedobs\")\n",
    "row = cur.fetchone()\n",
    "num_unique_accel_unconflicted_obs = row['count']\n",
    "\n",
    "data = [[f\"Num Raw Accel Records\",f\"{num_raw_accel_recs:,}\"],\n",
    "        [f\"Num Unique Accel Observations\",f\"{num_unique_accel_obs:,}\"],\n",
    "        [f\"Num Unique Accel Singleton Observations\",f\"{num_unique_accel_singleton_obs:,}\"],\n",
    "        [f\"Num Unique Accel Unconflicted Observations\",f\"{num_unique_accel_unconflicted_obs:,}\"],\n",
    "       ]\n",
    "\n",
    "print(\"Figure: Accel Record Counts:\")\n",
    "print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting observations per DC\n",
    "Now that we have filtered out all the duplicated and/or conflicted records, we have a set of unique user_id/record_time pairs, which we refer to as ObservationIDs. The next step is to group these into unique DCs. This is done by rounding the timestamps back to the beginning of the 5-minute window in which they occur.\n",
    "\n",
    "We compute these once, based on the unfiltered unique observations, and then again with only the singleton observations (which filter out duplicate records, including both conflicted and singleton-equivalent categories.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T20:49:42.275790Z",
     "start_time": "2020-08-11T20:49:40.529462Z"
    },
    "scrolled": true,
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Some GPS DCs with Observation Counts\n",
      "  recs_per_dc    user_id  record_time_max                   dc\n",
      "-------------  ---------  --------------------------------  ----------------\n",
      "           81       1030  2019-01-21 23:44:38.122000-08:00  2019-01-21 23:40\n",
      "            5       1030  2019-01-21 23:47:37.521000-08:00  2019-01-21 23:45\n",
      "            5       1030  2019-01-21 23:54:11.278000-08:00  2019-01-21 23:50\n",
      "            7       1030  2019-01-21 23:59:48.423000-08:00  2019-01-21 23:55\n",
      "            9       1030  2019-01-22 00:03:48.406000-08:00  2019-01-22 00:0\n",
      "            5       1030  2019-01-22 00:14:48.384000-08:00  2019-01-22 00:10\n",
      "            7       1030  2019-01-22 00:19:48.383000-08:00  2019-01-22 00:15\n",
      "            4       1030  2019-01-22 00:24:44.539000-08:00  2019-01-22 00:20\n",
      "           88       1030  2019-01-22 00:29:42.110000-08:00  2019-01-22 00:25\n",
      "           90       1030  2019-01-22 00:34:42.098000-08:00  2019-01-22 00:30\n",
      "\n",
      "\n",
      "\n",
      "Figure: Some GPS Singleton DCs with Observation Counts\n",
      "  recs_per_dc    user_id  record_time_max                   dc\n",
      "-------------  ---------  --------------------------------  ----------------\n",
      "           76       1030  2019-01-21 23:44:38.122000-08:00  2019-01-21 23:40\n",
      "            3       1030  2019-01-21 23:47:37.521000-08:00  2019-01-21 23:45\n",
      "            3       1030  2019-01-21 23:54:11.278000-08:00  2019-01-21 23:50\n",
      "            6       1030  2019-01-21 23:59:48.423000-08:00  2019-01-21 23:55\n",
      "            8       1030  2019-01-22 00:03:48.406000-08:00  2019-01-22 00:0\n",
      "            4       1030  2019-01-22 00:14:48.384000-08:00  2019-01-22 00:10\n",
      "            6       1030  2019-01-22 00:19:48.383000-08:00  2019-01-22 00:15\n",
      "            3       1030  2019-01-22 00:24:44.539000-08:00  2019-01-22 00:20\n",
      "           88       1030  2019-01-22 00:29:42.110000-08:00  2019-01-22 00:25\n",
      "           85       1030  2019-01-22 00:34:42.098000-08:00  2019-01-22 00:30\n",
      "\n",
      "\n",
      "\n",
      "Figure: Some GPS Unconflicted DCs with Observation Counts\n",
      "  recs_per_dc    user_id  record_time_max                   dc\n",
      "-------------  ---------  --------------------------------  ----------------\n",
      "           81       1030  2019-01-21 23:44:38.122000-08:00  2019-01-21 23:40\n",
      "            3       1030  2019-01-21 23:47:37.521000-08:00  2019-01-21 23:45\n",
      "            3       1030  2019-01-21 23:54:11.278000-08:00  2019-01-21 23:50\n",
      "            6       1030  2019-01-21 23:59:48.423000-08:00  2019-01-21 23:55\n",
      "            8       1030  2019-01-22 00:03:48.406000-08:00  2019-01-22 00:0\n",
      "            4       1030  2019-01-22 00:14:48.384000-08:00  2019-01-22 00:10\n",
      "            6       1030  2019-01-22 00:19:48.383000-08:00  2019-01-22 00:15\n",
      "            3       1030  2019-01-22 00:24:44.539000-08:00  2019-01-22 00:20\n",
      "           88       1030  2019-01-22 00:29:42.110000-08:00  2019-01-22 00:25\n",
      "           90       1030  2019-01-22 00:34:42.098000-08:00  2019-01-22 00:30\n"
     ]
    }
   ],
   "source": [
    "# Count GPS observations per DC\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS level_0.tmpgpsdcs AS\n",
    "        SELECT count(1) recs_per_dc, user_id,\n",
    "               max(record_time) as record_time_max,\n",
    "               to_char(record_time, 'YYYY-MM-DD HH24:') || (trunc((extract(minute from record_time)/5))*5)::text as dc \n",
    "        FROM level_0.tmpuniqgpsobs\n",
    "        GROUP BY user_id, dc;\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    # and add an index\n",
    "    cur.execute(\"CREATE INDEX ON level_0.tmpgpsdcs (user_id, dc)\")\n",
    "    \n",
    "    cur.execute(\"SELECT * FROM level_0.tmpgpsdcs LIMIT 10;\")\n",
    "    rows = cur.fetchall()\n",
    "    print(\"Figure: Some GPS DCs with Observation Counts\")\n",
    "    print(tabulate(rows, headers='keys'))\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS level_0.tmpgpssingletondcs AS\n",
    "        SELECT count(1) recs_per_dc, user_id,\n",
    "               max(record_time) as record_time_max,\n",
    "               to_char(record_time, 'YYYY-MM-DD HH24:') || (trunc((extract(minute from record_time)/5))*5)::text as dc \n",
    "        FROM level_0.tmpuniqgpssingletonobs\n",
    "        GROUP BY user_id, dc;\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    # and add an index\n",
    "    cur.execute(\"CREATE INDEX ON level_0.tmpgpssingletondcs (user_id, dc)\")\n",
    "    \n",
    "    cur.execute(\"SELECT * FROM level_0.tmpgpssingletondcs LIMIT 10;\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Figure: Some GPS Singleton DCs with Observation Counts\")\n",
    "    print(tabulate(rows, headers='keys'))\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS level_0.tmpgpsunconflicteddcs AS\n",
    "        SELECT count(1) recs_per_dc, user_id,\n",
    "               max(record_time) as record_time_max,\n",
    "               to_char(record_time, 'YYYY-MM-DD HH24:') || (trunc((extract(minute from record_time)/5))*5)::text as dc \n",
    "        FROM level_0.tmpuniqgpsunconflictedobs\n",
    "        GROUP BY user_id, dc;\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    # and add an index\n",
    "    cur.execute(\"CREATE INDEX ON level_0.tmpgpsunconflicteddcs (user_id, dc)\")\n",
    "    \n",
    "    cur.execute(\"SELECT * FROM level_0.tmpgpsunconflicteddcs LIMIT 10;\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Figure: Some GPS Unconflicted DCs with Observation Counts\")\n",
    "    print(tabulate(rows, headers='keys'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T20:50:02.616959Z",
     "start_time": "2020-08-11T20:50:02.265989Z"
    },
    "scrolled": true,
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: GPS DC Counts:\n",
      "---------------------------------  ---------\n",
      "                      Num GPS Obs  8,131,279\n",
      "                      Num GPS DCs    663,625\n",
      "         Num GPS Singleton DC Obs  7,729,649\n",
      "            Num GPS Singleton DCs    515,879\n",
      "      Num GPS Unconflicted DC Obs  7,791,943\n",
      "         Num GPS Unconflicted DCs    521,634\n",
      "\n",
      "      Fewest GPS Raw DCs Per User          1\n",
      "        Most GPS Raw DCs Per User      8,504\n",
      "         Avg GPS Raw DCs Per User     4394.9\n",
      "\n",
      "Fewest GPS Singleton DCs Per User          1\n",
      "  Most GPS Singleton DCs Per User      8,418\n",
      "   Avg GPS Singleton DCs Per User     3416.4\n",
      "---------------------------------  ---------\n"
     ]
    }
   ],
   "source": [
    "    # Now count both tables to see how many DCs we ended up with\n",
    "    cur.execute(\"SELECT count(1), sum(recs_per_dc) from level_0.tmpgpsdcs\")\n",
    "    row = cur.fetchone()\n",
    "    num_gps_dcs = row['count']\n",
    "    num_gps_dc_obs = row['sum']\n",
    "\n",
    "    cur.execute(\"SELECT count(1), sum(recs_per_dc) from level_0.tmpgpssingletondcs\")\n",
    "    row = cur.fetchone()\n",
    "    num_gps_singleton_dcs = row['count']\n",
    "    num_gps_singleton_dc_obs = row['sum']\n",
    "\n",
    "    cur.execute(\"SELECT count(1), sum(recs_per_dc) from level_0.tmpgpsunconflicteddcs\")\n",
    "    row = cur.fetchone()\n",
    "    num_gps_unconflicted_dcs = row['count']\n",
    "    num_gps_unconflicted_dc_obs = row['sum']\n",
    "\n",
    "    sql = \"\"\"SELECT min(count) fewest_dcs, max(count) most_dcs, avg(count) avg_dcs\n",
    "             FROM (\n",
    "                 SELECT count(1), user_id, min(dc)\n",
    "                 FROM level_0.tmpgpsdcs\n",
    "                 GROUP BY user_id\n",
    "             ) countdcs\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    fewest_gps_dcs_per_user = row['fewest_dcs']\n",
    "    most_gps_dcs_per_user = row['most_dcs']\n",
    "    avg_gps_dcs_per_user = row['avg_dcs']\n",
    "\n",
    "    sql = \"\"\"SELECT min(count) fewest_dcs, max(count) most_dcs, avg(count) avg_dcs\n",
    "             FROM (\n",
    "                 SELECT count(1), user_id, min(dc)\n",
    "                 FROM level_0.tmpgpssingletondcs\n",
    "                 GROUP BY user_id\n",
    "             ) countdcs\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    fewest_singleton_gps_dcs_per_user = row['fewest_dcs']\n",
    "    most_singleton_gps_dcs_per_user = row['most_dcs']\n",
    "    avg_singleton_gps_dcs_per_user = row['avg_dcs']\n",
    "\n",
    "    data = [[f\"Num GPS Obs\", f\"{num_gps_dc_obs:,}\"],\n",
    "            [f\"Num GPS DCs\", f\"{num_gps_dcs:,}\"],\n",
    "            [f\"Num GPS Singleton DC Obs\", f\"{num_gps_singleton_dc_obs:,}\"],\n",
    "            [f\"Num GPS Singleton DCs\", f\"{num_gps_singleton_dcs:,}\"],\n",
    "            [f\"Num GPS Unconflicted DC Obs\", f\"{num_gps_unconflicted_dc_obs:,}\"],\n",
    "            [f\"Num GPS Unconflicted DCs\", f\"{num_gps_unconflicted_dcs:,}\"],\n",
    "            ['',''],\n",
    "            [f\"Fewest GPS Unfiltered DCs Per User\", f\"{fewest_gps_dcs_per_user:,}\"],\n",
    "            [f\"Most GPS Unfiltered DCs Per User\", f\"{most_gps_dcs_per_user:,}\"],\n",
    "            [f\"Avg GPS Unfiltered DCs Per User\", f\"{avg_gps_dcs_per_user:0.1f}\"],\n",
    "            ['',''],\n",
    "            [f\"Fewest GPS Singleton DCs Per User\", f\"{fewest_singleton_gps_dcs_per_user:,}\"],\n",
    "            [f\"Most GPS Singleton DCs Per User\", f\"{most_singleton_gps_dcs_per_user:,}\"],\n",
    "            [f\"Avg GPS Singleton DCs Per User\", f\"{avg_singleton_gps_dcs_per_user:0.1f}\"],\n",
    "            ]\n",
    "    print(\"Figure: GPS DC Counts:\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T20:53:41.428450Z",
     "start_time": "2020-08-11T20:53:39.190930Z"
    },
    "scrolled": true,
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Some XL DCs with Observation Counts\n",
      "  recs_per_dc    user_id  record_time_max                   dc\n",
      "-------------  ---------  --------------------------------  ----------------\n",
      "          514       1030  2019-01-21 23:39:59.985000-08:00  2019-01-21 23:35\n",
      "         3389       1030  2019-01-21 23:42:38.019000-08:00  2019-01-21 23:40\n",
      "         2208       1030  2019-01-21 23:47:38.036000-08:00  2019-01-21 23:45\n",
      "          978       1030  2019-01-21 23:52:38.307000-08:00  2019-01-21 23:50\n",
      "          979       1030  2019-01-21 23:57:38.346000-08:00  2019-01-21 23:55\n",
      "          979       1030  2019-01-22 00:02:38.368000-08:00  2019-01-22 00:0\n",
      "          980       1030  2019-01-22 00:12:38.448000-08:00  2019-01-22 00:10\n",
      "          979       1030  2019-01-22 00:17:38.457000-08:00  2019-01-22 00:15\n",
      "          976       1030  2019-01-22 00:22:38.444000-08:00  2019-01-22 00:20\n",
      "         2974       1030  2019-01-22 00:29:12.322000-08:00  2019-01-22 00:25\n",
      "\n",
      "\n",
      "\n",
      "Figure: Some XL Singleton DCs with Observation Counts\n",
      "  recs_per_dc    user_id  record_time_max                   dc\n",
      "-------------  ---------  --------------------------------  ----------------\n",
      "          511       1030  2019-01-21 23:39:59.985000-08:00  2019-01-21 23:35\n",
      "         3380       1030  2019-01-21 23:42:38.019000-08:00  2019-01-21 23:40\n",
      "         2208       1030  2019-01-21 23:47:38.036000-08:00  2019-01-21 23:45\n",
      "          978       1030  2019-01-21 23:52:38.307000-08:00  2019-01-21 23:50\n",
      "          979       1030  2019-01-21 23:57:38.346000-08:00  2019-01-21 23:55\n",
      "          979       1030  2019-01-22 00:02:38.368000-08:00  2019-01-22 00:0\n",
      "          980       1030  2019-01-22 00:12:38.448000-08:00  2019-01-22 00:10\n",
      "          979       1030  2019-01-22 00:17:38.457000-08:00  2019-01-22 00:15\n",
      "          976       1030  2019-01-22 00:22:38.444000-08:00  2019-01-22 00:20\n",
      "         2971       1030  2019-01-22 00:29:12.322000-08:00  2019-01-22 00:25\n",
      "\n",
      "\n",
      "\n",
      "Figure: Some XL Unconflicted DCs with Observation Counts\n",
      "  recs_per_dc    user_id  record_time_max                   dc\n",
      "-------------  ---------  --------------------------------  ----------------\n",
      "          511       1030  2019-01-21 23:39:59.985000-08:00  2019-01-21 23:35\n",
      "         3380       1030  2019-01-21 23:42:38.019000-08:00  2019-01-21 23:40\n",
      "         2208       1030  2019-01-21 23:47:38.036000-08:00  2019-01-21 23:45\n",
      "          978       1030  2019-01-21 23:52:38.307000-08:00  2019-01-21 23:50\n",
      "          979       1030  2019-01-21 23:57:38.346000-08:00  2019-01-21 23:55\n",
      "          979       1030  2019-01-22 00:02:38.368000-08:00  2019-01-22 00:0\n",
      "          980       1030  2019-01-22 00:12:38.448000-08:00  2019-01-22 00:10\n",
      "          979       1030  2019-01-22 00:17:38.457000-08:00  2019-01-22 00:15\n",
      "          976       1030  2019-01-22 00:22:38.444000-08:00  2019-01-22 00:20\n",
      "         2971       1030  2019-01-22 00:29:12.322000-08:00  2019-01-22 00:25\n"
     ]
    }
   ],
   "source": [
    "# Count Accel observations per DC\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS level_0.tmpacceldcs AS\n",
    "            SELECT count(1) recs_per_dc, user_id,\n",
    "                   max(record_time) as record_time_max,\n",
    "                   to_char(record_time, 'YYYY-MM-DD HH24:') || (trunc((extract(minute from record_time)/5))*5)::text as dc \n",
    "            FROM level_0.tmpuniqaccelobs\n",
    "            GROUP BY user_id, dc;\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    # and add an index\n",
    "    cur.execute(\"CREATE INDEX ON level_0.tmpacceldcs (user_id, dc)\")\n",
    "    \n",
    "    cur.execute(\"SELECT * FROM level_0.tmpacceldcs LIMIT 10;\")\n",
    "    rows = cur.fetchall()\n",
    "    print(\"Figure: Some XL DCs with Observation Counts\")\n",
    "    print(tabulate(rows, headers='keys'))\n",
    "\n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS level_0.tmpaccelsingletondcs AS\n",
    "            SELECT count(1) recs_per_dc, user_id,\n",
    "                   max(record_time) as record_time_max,\n",
    "                   to_char(record_time, 'YYYY-MM-DD HH24:') || (trunc((extract(minute from record_time)/5))*5)::text as dc \n",
    "            FROM level_0.tmpuniqaccelsingletonobs\n",
    "            GROUP BY user_id, dc;\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    # and add an index\n",
    "    cur.execute(\"CREATE INDEX ON level_0.tmpaccelsingletondcs (user_id, dc)\")\n",
    "    \n",
    "    cur.execute(\"SELECT * FROM level_0.tmpaccelsingletondcs LIMIT 10;\")\n",
    "    rows = cur.fetchall()\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Figure: Some XL Singleton DCs with Observation Counts\")\n",
    "    print(tabulate(rows, headers='keys'))\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS level_0.tmpaccelunconflicteddcs AS\n",
    "        SELECT count(1) recs_per_dc, user_id,\n",
    "               max(record_time) as record_time_max,\n",
    "               to_char(record_time, 'YYYY-MM-DD HH24:') || (trunc((extract(minute from record_time)/5))*5)::text as dc \n",
    "        FROM level_0.tmpuniqaccelunconflictedobs\n",
    "        GROUP BY user_id, dc;\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    # and add an index\n",
    "    cur.execute(\"CREATE INDEX ON level_0.tmpaccelunconflicteddcs (user_id, dc)\")\n",
    "    \n",
    "    cur.execute(\"SELECT * FROM level_0.tmpaccelunconflicteddcs LIMIT 10;\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Figure: Some XL Unconflicted DCs with Observation Counts\")\n",
    "    print(tabulate(rows, headers='keys'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T20:53:55.608749Z",
     "start_time": "2020-08-11T20:53:55.275975Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: XL DC Counts:\n",
      "--------------------------------------  -----------\n",
      "                Num Accel Observations  878,955,386\n",
      "                         Num Accel DCs      715,499\n",
      "   Num Accel Singleton DC Observations  832,152,864\n",
      "               Num Accel Singleton DCs      715,211\n",
      "Num Accel Unconflicted DC Observations  836,034,501\n",
      "            Num Accel Unconflicted DCs      715,356\n",
      "\n",
      "                Fewest XL DCs Per User            1\n",
      "                  Most XL DCs Per User        8,590\n",
      "                   Avg XL DCs Per User       4707.2\n",
      "\n",
      "      Fewest XL Singleton DCs Per User            1\n",
      "        Most XL Singleton DCs Per User        8,590\n",
      "         Avg XL Singleton DCs Per User       4705.3\n",
      "--------------------------------------  -----------\n"
     ]
    }
   ],
   "source": [
    "    # Now count both tables to see how many DCs we ended up with\n",
    "    cur.execute(\"SELECT count(1), sum(recs_per_dc) from level_0.tmpacceldcs\")\n",
    "    row = cur.fetchone()\n",
    "    num_accel_dcs = row['count']\n",
    "    num_accel_dc_obs = row['sum']\n",
    "\n",
    "    cur.execute(\"SELECT count(1), sum(recs_per_dc) from level_0.tmpaccelsingletondcs\")\n",
    "    row = cur.fetchone()\n",
    "    num_accel_singleton_dcs = row['count']\n",
    "    num_accel_singleton_dc_obs = row['sum']\n",
    "\n",
    "    cur.execute(\"SELECT count(1), sum(recs_per_dc) from level_0.tmpaccelunconflicteddcs\")\n",
    "    row = cur.fetchone()\n",
    "    num_accel_unconflicted_dcs = row['count']\n",
    "    num_accel_unconflicted_dc_obs = row['sum']\n",
    "\n",
    "    sql = \"\"\"SELECT min(count) fewest_dcs, max(count) most_dcs, avg(count) avg_dcs\n",
    "             FROM (\n",
    "                 SELECT count(1), user_id, min(dc)\n",
    "                 FROM level_0.tmpacceldcs\n",
    "                 GROUP BY user_id\n",
    "             ) countdcs\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    fewest_accel_dcs_per_user = row['fewest_dcs']\n",
    "    most_accel_dcs_per_user = row['most_dcs']\n",
    "    avg_accel_dcs_per_user = row['avg_dcs']\n",
    "\n",
    "    sql = \"\"\"SELECT min(count) fewest_dcs, max(count) most_dcs, avg(count) avg_dcs\n",
    "             FROM (\n",
    "                 SELECT count(1), user_id, min(dc)\n",
    "                 FROM level_0.tmpaccelsingletondcs\n",
    "                 GROUP BY user_id\n",
    "             ) countdcs\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    fewest_singleton_accel_dcs_per_user = row['fewest_dcs']\n",
    "    most_singleton_accel_dcs_per_user = row['most_dcs']\n",
    "    avg_singleton_accel_dcs_per_user = row['avg_dcs']\n",
    "\n",
    "    data = [[f\"Num Accel Observations\",f\"{num_accel_dc_obs:,}\"],\n",
    "            [f\"Num Accel DCs\",f\"{num_accel_dcs:,}\"],\n",
    "            [f\"Num Accel Singleton DC Observations\",f\"{num_accel_singleton_dc_obs:,}\"],\n",
    "            [f\"Num Accel Singleton DCs\",f\"{num_accel_singleton_dcs:,}\"],\n",
    "            [f\"Num Accel Unconflicted DC Observations\",f\"{num_accel_unconflicted_dc_obs:,}\"],\n",
    "            [f\"Num Accel Unconflicted DCs\",f\"{num_accel_unconflicted_dcs:,}\"],\n",
    "            ['',''],\n",
    "            [f\"Fewest XL DCs Per User\", f\"{fewest_accel_dcs_per_user:,}\"],\n",
    "            [f\"Most XL DCs Per User\", f\"{most_accel_dcs_per_user:,}\"],\n",
    "            [f\"Avg XL DCs Per User\", f\"{avg_accel_dcs_per_user:0.1f}\"],\n",
    "            ['',''],\n",
    "            [f\"Fewest XL Singleton DCs Per User\", f\"{fewest_singleton_accel_dcs_per_user:,}\"],\n",
    "            [f\"Most XL Singleton DCs Per User\", f\"{most_singleton_accel_dcs_per_user:,}\"],\n",
    "            [f\"Avg XL Singleton DCs Per User\", f\"{avg_singleton_accel_dcs_per_user:0.1f}\"],\n",
    "           ]\n",
    "    print(\"Figure: XL DC Counts:\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can filter the above results to only those DCs that meet the threshold for viability and see how many are lost. (GPS requires 10+ obs/dc to be viable. XL requires 120.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T19:20:08.461228Z",
     "start_time": "2020-08-11T19:20:08.088035Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Count the viable GPS DCs\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT  COUNT(1) as count\n",
    "        FROM level_0.tmpgpsdcs\n",
    "        WHERE recs_per_dc >= 10;\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    viable_gps_dcs = row['count']\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT count(1) as count\n",
    "        FROM level_0.tmpgpssingletondcs\n",
    "        WHERE recs_per_dc >= 10;\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    viable_gps_singleton_dcs = row['count']\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT count(1) as count\n",
    "        FROM level_0.tmpgpsunconflicteddcs\n",
    "        WHERE recs_per_dc >= 10;\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    viable_gps_unconflicted_dcs = row['count']\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT count(1) as num_users\n",
    "        FROM (\n",
    "            SELECT count(1) as count\n",
    "            FROM level_0.tmpgpsdcs\n",
    "            WHERE recs_per_dc >= 10\n",
    "            GROUP BY user_id\n",
    "            ) users_with_viable_dcs\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    num_users_w_viable_gps_dcs = row['num_users']\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT count(1) as num_users\n",
    "        FROM (\n",
    "            SELECT count(1) as count\n",
    "            FROM level_0.tmpgpssingletondcs\n",
    "            WHERE recs_per_dc >= 10\n",
    "            GROUP BY user_id\n",
    "            ) users_with_viable_dcs\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    num_users_w_viable_singleton_gps_dcs = row['num_users']\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT count(1) as num_users\n",
    "        FROM (\n",
    "            SELECT count(1) as count\n",
    "            FROM level_0.tmpgpsunconflicteddcs\n",
    "            WHERE recs_per_dc >= 10\n",
    "            GROUP BY user_id\n",
    "            ) users_with_viable_dcs\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    num_users_w_viable_unconflicted_gps_dcs = row['num_users']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T19:22:30.919665Z",
     "start_time": "2020-08-11T19:22:30.394587Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Count the viable Accel DCs\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT count(1) as count\n",
    "        FROM level_0.tmpacceldcs\n",
    "        WHERE recs_per_dc >= 120\n",
    "        LIMIT 10;\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    viable_accel_dcs = row['count']\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT count(1) as count\n",
    "        FROM level_0.tmpaccelsingletondcs\n",
    "        WHERE recs_per_dc >= 120\n",
    "        LIMIT 10;\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    viable_accel_singleton_dcs = row['count']\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT count(1) as count\n",
    "        FROM level_0.tmpaccelunconflicteddcs\n",
    "        WHERE recs_per_dc >= 120\n",
    "        LIMIT 10;\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    viable_accel_unconflicted_dcs = row['count']\n",
    "        \n",
    "    sql = \"\"\"\n",
    "        SELECT count(1) as num_users\n",
    "        FROM (\n",
    "            SELECT count(1) as count\n",
    "            FROM level_0.tmpacceldcs\n",
    "            WHERE recs_per_dc >= 10\n",
    "            GROUP BY user_id\n",
    "            ) users_with_viable_dcs\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    num_users_w_viable_accel_dcs = row['num_users']\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT count(1) as num_users\n",
    "        FROM (\n",
    "            SELECT count(1) as count\n",
    "            FROM level_0.tmpaccelsingletondcs\n",
    "            WHERE recs_per_dc >= 10\n",
    "            GROUP BY user_id\n",
    "            ) users_with_viable_dcs\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    num_users_w_viable_singleton_accel_dcs = row['num_users']\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT count(1) as num_users\n",
    "        FROM (\n",
    "            SELECT count(1) as count\n",
    "            FROM level_0.tmpaccelunconflicteddcs\n",
    "            WHERE recs_per_dc >= 10\n",
    "            GROUP BY user_id\n",
    "            ) users_with_viable_dcs\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    num_users_w_viable_unconflicted_accel_dcs = row['num_users']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "The objective is to assess how many usable duty cycles of data are lost if we eliminate all duplicated records. (Maximum expectated count is based on 151 users over 28 days of study, 24-hours per day, with a 1-in-5 duty cycle.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T21:08:03.858071Z",
     "start_time": "2020-08-11T21:08:03.835249Z"
    },
    "scrolled": false,
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  ---------\n",
      "                       Max Expected DC Count  1,217,664\n",
      "\n",
      "                                 Num GPS DCs    663,625\n",
      "        Viable GPS DCs before Filtering Dups     81,274\n",
      "        Viable GPS DCs after Filtering Confs     79,044\n",
      "         Viable GPS DCs after Filtering Dups     78,564\n",
      "                          Conf Loss Rate GPS       2.7%\n",
      "                           Dup Loss Rate GPS       3.3%\n",
      "\n",
      "                                  Num XL DCs    715,499\n",
      "         Viable XL DCs before Filtering Dups    712,318\n",
      "         Viable XL DCs after Filtering Confs    704,501\n",
      "          Viable XL DCs after Filtering Dups    703,401\n",
      "                           Conf Loss Rate XL       1.1%\n",
      "                            Dup Loss Rate XL       1.3%\n",
      "\n",
      "Users w Viable GPS DCs Before Filtering Dups        145\n",
      "Users w Viable GPS DCs After Filtering Confs        145\n",
      " Users w Viable GPS DCs After Filtering Dups        145\n",
      "\n",
      " Users w Viable XL DCs Before Filtering Dups        152\n",
      " Users w Viable XL DCs After Filtering Confs        152\n",
      "  Users w Viable XL DCs After Filtering Dups        152\n",
      "\n",
      "     Avg Viable GPS DCs per user, unfiltered      560.5\n",
      "   Avg Viable GPS DCs per user, unconflicted      545.1\n",
      "     Avg Viable GPS DCs per user, singletons      541.8\n",
      "\n",
      "      Avg Viable XL DCs per user, unfiltered     4686.3\n",
      "    Avg Viable XL DCs per user, unconflicted     4634.9\n",
      "      Avg Viable XL DCs per user, singletons     4627.6\n",
      "--------------------------------------------  ---------\n"
     ]
    }
   ],
   "source": [
    "# Estimate how many DCs we should be expecting\n",
    "study_length_days = 28\n",
    "duty_cycles_per_hour = 12\n",
    "num_users = 151\n",
    "expected_dc_count = num_users * study_length_days * 24 * duty_cycles_per_hour\n",
    "\n",
    "#print(tabulate(data, stralign='right'))\n",
    "comparables = [['Max Expected DC Count', f\"{expected_dc_count:,}\"],\n",
    "   ['',''],\n",
    "   ['Num GPS DCs', f\"{num_gps_dcs:,}\"],\n",
    "\n",
    "   ['Viable GPS DCs before Filtering Dups',f\"{viable_gps_dcs:,}\"],\n",
    "   ['Viable GPS DCs after Filtering Confs', f\"{viable_gps_unconflicted_dcs:,}\"],\n",
    "   ['Viable GPS DCs after Filtering Dups', f\"{viable_gps_singleton_dcs:,}\"],\n",
    "   ['Conf Loss Rate GPS', f\"{100*(1.0 - (viable_gps_unconflicted_dcs/float(viable_gps_dcs))):0.1f}%\"],\n",
    "   ['Dup Loss Rate GPS', f\"{100*(1.0 - (viable_gps_singleton_dcs/float(viable_gps_dcs))):0.1f}%\"],\n",
    "   ['',''],\n",
    "   ['Num XL DCs', f\"{num_accel_dcs:,}\"],\n",
    "   ['Viable XL DCs before Filtering Dups', f\"{viable_accel_dcs:,}\"],\n",
    "   ['Viable XL DCs after Filtering Confs', f\"{viable_accel_unconflicted_dcs:,}\"],\n",
    "   ['Viable XL DCs after Filtering Dups', f\"{viable_accel_singleton_dcs:,}\"],\n",
    "   ['Conf Loss Rate XL', f\"{100*(1.0 - (viable_accel_unconflicted_dcs/float(viable_accel_dcs))):0.1f}%\"],\n",
    "   ['Dup Loss Rate XL', f\"{100*(1.0 - (viable_accel_singleton_dcs/float(viable_accel_dcs))):0.1f}%\"],\n",
    "   ['',''],\n",
    "   ['Users w Viable GPS DCs Before Filtering Dups', num_users_w_viable_gps_dcs],\n",
    "   ['Users w Viable GPS DCs After Filtering Confs', num_users_w_viable_unconflicted_gps_dcs],\n",
    "   ['Users w Viable GPS DCs After Filtering Dups', num_users_w_viable_singleton_gps_dcs],\n",
    "   ['',''],\n",
    "   ['Users w Viable XL DCs Before Filtering Dups', num_users_w_viable_accel_dcs],\n",
    "   ['Users w Viable XL DCs After Filtering Confs', num_users_w_viable_unconflicted_accel_dcs],\n",
    "   ['Users w Viable XL DCs After Filtering Dups', num_users_w_viable_singleton_accel_dcs],\n",
    "   ['',''],               \n",
    "   ['Avg Viable GPS DCs per user, unfiltered', f\"{viable_gps_dcs/float(num_users_w_viable_gps_dcs):0.1f}\"],\n",
    "   ['Avg Viable GPS DCs per user, unconflicted', f\"{viable_gps_unconflicted_dcs/float(num_users_w_viable_unconflicted_gps_dcs):0.1f}\"],\n",
    "   ['Avg Viable GPS DCs per user, singletons', f\"{viable_gps_singleton_dcs/float(num_users_w_viable_singleton_gps_dcs):0.1f}\"],\n",
    "   ['',''],               \n",
    "   ['Avg Viable XL DCs per user, unfiltered', f\"{viable_accel_dcs/float(num_users_w_viable_accel_dcs):0.1f}\"],\n",
    "   ['Avg Viable XL DCs per user, unconflicted', f\"{viable_accel_unconflicted_dcs/float(num_users_w_viable_unconflicted_accel_dcs):0.1f}\"],\n",
    "   ['Avg Viable XL DCs per user, singletons', f\"{viable_accel_singleton_dcs/float(num_users_w_viable_singleton_accel_dcs):0.1f}\"],\n",
    "\n",
    "]\n",
    "print(tabulate(comparables,stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, this means that if we were to filter out all of the duplicated and/or conflicted raw samples, we would have:\n",
    "\n",
    "  - The same number of users represented in the data\n",
    "  - 3.3% fewer viable duty cycles in GPS data\n",
    "  - 1.3% fewer viable duty cycles in XL data\n",
    "  \n",
    "If we filter only the actually conflicted duplicates, the results are slightly better:\n",
    "  - The same number of users represented in the data\n",
    "  - 2.7% fewer viable duty cycles in GPS data\n",
    "  - 1.1% fewer viable duty cycles in XL data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronization of Observations Between GPS and XL\n",
    "\n",
    "For some kinds of analysis, it is important that the GPS and XL sensor data be coordinated so that they are both measuring the same period of activity. To know whether this dataset will support such analysis, we need to measure the degree to which timestamps match between the two tables. For this, we'll count the proportion of ObservationIDs in the GPS table that have matching ObservationIDs in the Accel table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Joins\n",
    "Since the GPS was sampled at approx. 1Hz and the XL data was at 20Hz, the theoretical best case would show 20 times as many XL observations as GPS, with every 20th observation sharing a timestamp. We don't expect to see perfect coherence, but we don't know how much coherence there might actually be. To assess this, we count the number of successful left joins between the two tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T19:39:07.849158Z",
     "start_time": "2020-08-09T19:31:54.802291Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Count the left join matches between GPS and Accel tables\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT COUNT(1)\n",
    "        FROM level_0.tmpuniqgpsobs as gps\n",
    "        INNER JOIN level_0.tmpuniqaccelobs as accel\n",
    "            ON gps.record_time = accel.record_time \n",
    "            AND gps.user_id = accel.user_id\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    num_matches = row['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T19:49:15.830172Z",
     "start_time": "2020-08-09T19:49:15.820519Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Comparison of Unique Observation Matches w/o Filtering:\n",
      "--------------------  -----------\n",
      "Num GPS Observations    8,131,279\n",
      "  Expected Accel Obs  162,625,580\n",
      "    Actual Accel Obs  878,955,386\n",
      "       % of Expected       540.5%\n",
      "   Exact Obs Matches      350,356\n",
      "          Match Rate         4.3%\n",
      "--------------------  -----------\n"
     ]
    }
   ],
   "source": [
    "print(\"Figure: Comparison of Unique Observation Matches w/o Filtering:\")\n",
    "table = [[\"Num GPS Observations\",f\"{num_unique_gps_obs:,}\"], \n",
    "          [\"Expected Accel Obs\",f\"{num_unique_gps_obs*20:,}\"], \n",
    "          [\"Actual Accel Obs\",f\"{num_unique_accel_obs:,}\"], \n",
    "          [\"% of Expected\",f\"{100*num_unique_accel_obs/(num_unique_gps_obs*20):0.1f}%\"], \n",
    "          [\"Exact Obs Matches\",f\"{num_matches:,}\"], \n",
    "          [\"Match Rate\",f\"{100*num_matches/float(num_unique_gps_obs):0.1f}%\"]]\n",
    "print(tabulate(table,stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now examine how those numbers change if we consider only the raw observations that are true singletons, filtering out all duplicated and/or conflicted samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T20:18:33.506259Z",
     "start_time": "2020-08-11T20:18:24.590712Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Count the left join matches between GPS and Accel tables using the singleton-only tables\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT COUNT(1)\n",
    "        FROM level_0.tmpuniqgpssingletonobs as gps\n",
    "        INNER JOIN level_0.tmpuniqaccelsingletonobs as accel\n",
    "            ON gps.record_time = accel.record_time \n",
    "            AND gps.user_id = accel.user_id\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    num_singleton_matches = row['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T20:18:49.231960Z",
     "start_time": "2020-08-11T20:18:49.222544Z"
    },
    "scrolled": true,
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Comparison of Unique Observation Matches with Full Duplicate Filtering:\n",
      "------------------  -----------\n",
      "      Num GPS Rows    7,729,649\n",
      "Expected Accel Obs  154,592,980\n",
      "  Actual Accel Obs  832,152,864\n",
      "     % of Expected       538.3%\n",
      " Exact Obs Matches      290,535\n",
      "        Match Rate         3.8%\n",
      "------------------  -----------\n"
     ]
    }
   ],
   "source": [
    "    print(\"Figure: Comparison of Unique Observation Matches with Full Duplicate Filtering:\")\n",
    "    table = [[\"Num GPS Rows\",f\"{num_unique_gps_singleton_obs:,}\"], \n",
    "             [\"Expected Accel Obs\",f\"{num_unique_gps_singleton_obs*20:,}\"], \n",
    "             [\"Actual Accel Obs\",f\"{num_unique_accel_singleton_obs:,}\"], \n",
    "             [\"% of Expected\",f\"{100*num_unique_accel_singleton_obs/(num_unique_gps_singleton_obs*20):0.1f}%\"], \n",
    "             [\"Exact Obs Matches\", f\"{num_singleton_matches:,}\"], \n",
    "             [\"Match Rate\",f\"{100*num_singleton_matches/float(num_unique_gps_singleton_obs):0.1f}%\"]]\n",
    "    print(tabulate(table,stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T20:19:29.216227Z",
     "start_time": "2020-08-11T20:19:15.212252Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Count the left join matches between GPS and Accel tables using the unconflicted tables\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT COUNT(1)\n",
    "        FROM level_0.tmpuniqgpsunconflictedobs as gps\n",
    "        INNER JOIN level_0.tmpuniqaccelunconflictedobs as accel\n",
    "            ON gps.record_time = accel.record_time \n",
    "            AND gps.user_id = accel.user_id\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    num_unconflicted_matches = row['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T20:19:32.378047Z",
     "start_time": "2020-08-11T20:19:32.368677Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Comparison of Unique Observation Matches with Conflict-only Filtering:\n",
      "------------------  -----------\n",
      "      Num GPS Rows    7,791,943\n",
      "Expected Accel Obs  155,838,860\n",
      "  Actual Accel Obs  836,034,501\n",
      "     % of Expected       536.5%\n",
      " Exact Obs Matches      292,714\n",
      "        Match Rate         3.8%\n",
      "------------------  -----------\n"
     ]
    }
   ],
   "source": [
    "    print(\"Figure: Comparison of Unique Observation Matches with Conflict-only Filtering:\")\n",
    "    table = [[\"Num GPS Rows\",f\"{num_unique_gps_unconflicted_obs:,}\"], \n",
    "             [\"Expected Accel Obs\",f\"{num_unique_gps_unconflicted_obs*20:,}\"], \n",
    "             [\"Actual Accel Obs\",f\"{num_unique_accel_unconflicted_obs:,}\"], \n",
    "             [\"% of Expected\",f\"{100*num_unique_accel_unconflicted_obs/(num_unique_gps_unconflicted_obs*20):0.1f}%\"], \n",
    "             [\"Exact Obs Matches\", f\"{num_unconflicted_matches:,}\"], \n",
    "             [\"Match Rate\",f\"{100*num_unconflicted_matches/float(num_unique_gps_unconflicted_obs):0.1f}%\"]]\n",
    "    print(tabulate(table,stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "source": [
    "### Preliminary Analysis\n",
    "There are a couple of findings here worthy of note:\n",
    "\n",
    "- The number of samples in the XL table seems unusually high, with more than 5 times the expected number of samples.\n",
    "- Only 4.3% of the GPS observation timestamps match between the two tables. (In theory, *all* of them should have corresponding XL records.)\n",
    "- The match rate falls to 3.8% for both fully and partially filtered duplicates. Whether this is a problem will have to be decided by the researchers \n",
    "  \n",
    "There are several possible explanations for the excess XL data. It's possible that the duplicate records are being created outside scheduled duty cycles. Another possible explanation would be an error in the sampling rate for either of the sensors. We'll dive into a little sidebar here to see if we can find an explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating Excess Data\n",
    "A number of probes are conducted in an attempt to find the source of the excess XL data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Was the data sampled at the correct frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T20:11:33.070822Z",
     "start_time": "2020-08-09T20:11:04.785912Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: GPS Sampling Intervals\n",
      "-------------  ------------------------\n",
      "       minlag            0:00:00.001000\n",
      "       maxlag  18 days, 18:15:34.740000\n",
      "       avglag            0:00:42.013975\n",
      "percentile_10            0:00:00.031000\n",
      "percentile_25            0:00:00.975000\n",
      "percentile_50            0:00:00.999000\n",
      "percentile_75            0:00:01.015000\n",
      "percentile_90            0:00:31.068000\n",
      "-------------  ------------------------\n"
     ]
    }
   ],
   "source": [
    "# Let's just confirm the typical lag between successive records in both GPS and XL tables\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT min(lag) as minlag, max(lag) as maxlag, avg(lag) as avglag,\n",
    "        percentile_disc(0.10) within group (order by lag asc) as percentile_10,\n",
    "        percentile_disc(0.25) within group (order by lag asc) as percentile_25,\n",
    "        percentile_disc(0.5) within group (order by lag asc) as percentile_50,\n",
    "        percentile_disc(0.75) within group (order by lag asc) as percentile_75,\n",
    "        percentile_disc(0.90) within group (order by lag asc) as percentile_90\n",
    "        FROM (\n",
    "            SELECT user_id, record_time, record_time - LAG(record_time)\n",
    "                OVER (\n",
    "                    PARTITION BY (user_id)\n",
    "                    ORDER BY (record_time)\n",
    "                    ) as lag\n",
    "            FROM level_0.tmpuniqgpsobs\n",
    "            ) lagcalc\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [[x,row[x]] for x in row.keys()]\n",
    "    print(\"Figure: GPS Sampling Intervals\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T20:44:00.435589Z",
     "start_time": "2020-08-09T20:11:46.497216Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: XL Sampling Intervals\n",
      "-------------  ------------------------\n",
      "       minlag            0:00:00.001000\n",
      "       maxlag  18 days, 18:14:53.337000\n",
      "       avglag            0:00:00.394966\n",
      "percentile_10            0:00:00.019000\n",
      "percentile_25            0:00:00.021000\n",
      "percentile_50            0:00:00.060000\n",
      "percentile_75            0:00:00.061000\n",
      "percentile_90            0:00:00.066000\n",
      "-------------  ------------------------\n"
     ]
    }
   ],
   "source": [
    "# And again for XL data\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT min(lag) as minlag, max(lag) as maxlag, avg(lag) as avglag,\n",
    "        percentile_disc(0.10) within group (order by lag asc) as percentile_10,\n",
    "        percentile_disc(0.25) within group (order by lag asc) as percentile_25,\n",
    "        percentile_disc(0.5) within group (order by lag asc) as percentile_50,\n",
    "        percentile_disc(0.75) within group (order by lag asc) as percentile_75,\n",
    "        percentile_disc(0.90) within group (order by lag asc) as percentile_90\n",
    "        FROM (\n",
    "            SELECT \n",
    "                user_id, \n",
    "                record_time, \n",
    "                record_time - LAG(record_time) OVER (\n",
    "                    PARTITION BY (user_id)\n",
    "                    ORDER BY (record_time)\n",
    "                ) as lag\n",
    "            FROM level_0.tmpuniqaccelobs\n",
    "            ) lagcalc\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [[x,row[x]] for x in row.keys()]\n",
    "    print(\"Figure: XL Sampling Intervals\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis, it appears that the GPS data is indeed sampled on the order of 1Hz and the XL data on 20Hz. So the 400% extra XL samples do not seem to be caused by a problematic sampling rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "source": [
    "#### Was XL data collected on a 1-in-1 duty cycle?\n",
    "The XL data is supposed to be collected for one minute in every 5, but since we haveg 5 times more data than expected, it raises the question of whether the duty cycle was functioning properly.\n",
    "\n",
    "To probe this, we'll conduct a similar lag test, but this time, we'll round the intervals to the nearest minute. The samples within a single DC will typically be spaced at 1Hz or 20Hz, so the lag between successive samples will be 0 minutes. But after excluding those, the next most frequent lag rate should be approx 4 minutes - the time between the last sample from one DC and the first sample of the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Let's examine the typical LAG MINUTES between successive observations in the GPS table\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT user_id, record_time_max, \n",
    "            (extract ('epoch' from (record_time_max - LAG(record_time_max)\n",
    "                OVER (\n",
    "                    PARTITION BY (user_id)\n",
    "                    ORDER BY (record_time_max)\n",
    "                    )))/60)::int as lagmin\n",
    "            FROM level_0.tmpgpsdcs\n",
    "            ) lagmincalc\n",
    "            LIMIT 10\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [[x,row[x]] for x in row.keys()]\n",
    "    print(\"Figure: GPS Duty Cycle Lag Minute Frequency\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T17:56:31.049689Z",
     "start_time": "2020-08-09T17:52:47.003Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Let's just confirm the typical lag between successive DCs in the GPS table\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT min(lag) as minlag, max(lag) as maxlag, avg(lag) as avglag,\n",
    "        percentile_disc(0.10) within group (order by lag asc) as percentile_10,\n",
    "        percentile_disc(0.25) within group (order by lag asc) as percentile_25,\n",
    "        percentile_disc(0.5) within group (order by lag asc) as percentile_50,\n",
    "        percentile_disc(0.75) within group (order by lag asc) as percentile_75,\n",
    "        percentile_disc(0.90) within group (order by lag asc) as percentile_90\n",
    "        FROM (\n",
    "            SELECT user_id, record_time_max, record_time_max - LAG(record_time_max)\n",
    "                OVER (\n",
    "                    PARTITION BY (user_id)\n",
    "                    ORDER BY (record_time_max)\n",
    "                    ) as lag\n",
    "            FROM level_0.tmpgpsdcs\n",
    "            ) lagcalc\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [[x,row[x]] for x in row.keys()]\n",
    "    print(\"Figure: GPS Duty Cycle Intervals\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T17:56:31.051591Z",
     "start_time": "2020-08-09T17:52:47.005Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# And repeat for the XL DC table\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT min(lag) as minlag, max(lag) as maxlag, avg(lag) as avglag,\n",
    "        percentile_disc(0.10) within group (order by lag asc) as percentile_10,\n",
    "        percentile_disc(0.25) within group (order by lag asc) as percentile_25,\n",
    "        percentile_disc(0.5) within group (order by lag asc) as percentile_50,\n",
    "        percentile_disc(0.75) within group (order by lag asc) as percentile_75,\n",
    "        percentile_disc(0.90) within group (order by lag asc) as percentile_90\n",
    "        FROM (\n",
    "            SELECT user_id, record_time_max, record_time_max - LAG(record_time_max)\n",
    "                OVER (\n",
    "                    PARTITION BY (user_id)\n",
    "                    ORDER BY (record_time_max)\n",
    "                    ) as lag\n",
    "            FROM level_0.tmpacceldcs\n",
    "            ) lagcalc\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [[x,row[x]] for x in row.keys()]\n",
    "    print(\"Figure: XL Duty Cycle Intervals\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll test this by binning the actual observation lags into minute-sized buckets and look for the most frequent lag sizes. Clearly, the 0-minutes bucket will be the most common, since samples within the duty cycle are sub-1-min in spacing, but presumably the next most common lag should be 4 minutes, between the last sample in one DC and the first sample in the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T15:03:15.097321Z",
     "start_time": "2020-08-10T15:03:09.299362Z"
    },
    "scrolled": true,
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: GPS Duty Cycle Intervals (in minutes)\n",
      "-------------  ----------\n",
      "       minlag      1\n",
      "       maxlag  27016\n",
      "       avglag      6.6991\n",
      "percentile_10      1\n",
      "percentile_25      2\n",
      "percentile_50      5\n",
      "percentile_75      5\n",
      "percentile_90      5\n",
      "-------------  ----------\n"
     ]
    }
   ],
   "source": [
    "# Let's just confirm the typical lag between successive DCs, first with GPS\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT min(lag) as minlag, max(lag) as maxlag, avg(lag) as avglag,\n",
    "        percentile_disc(0.10) within group (order by lag asc) as percentile_10,\n",
    "        percentile_disc(0.25) within group (order by lag asc) as percentile_25,\n",
    "        percentile_disc(0.5) within group (order by lag asc) as percentile_50,\n",
    "        percentile_disc(0.75) within group (order by lag asc) as percentile_75,\n",
    "        percentile_disc(0.90) within group (order by lag asc) as percentile_90\n",
    "        FROM (\n",
    "            SELECT user_id, record_time, (extract ('epoch' from (record_time - LAG(record_time)\n",
    "                OVER (\n",
    "                    PARTITION BY (user_id)\n",
    "                    ORDER BY (record_time)\n",
    "                    )))/60)::int as lag\n",
    "            FROM level_0.tmpuniqgpsobs\n",
    "            ) lagcalc\n",
    "        WHERE lag > 0\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [[x,row[x]] for x in row.keys()]\n",
    "    print(\"Figure: GPS Duty Cycle Intervals (in minutes)\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result demonstrates that the GPS table is indeed following the 1-in-5 duty cycle. Now we can apply the same test to the XL table and see if it does as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T15:14:06.814016Z",
     "start_time": "2020-08-10T15:03:54.048703Z"
    },
    "scrolled": true,
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: XL Duty Cycle Intervals (in minutes)\n",
      "-------------  -----------\n",
      "       minlag      1\n",
      "       maxlag  27015\n",
      "       avglag      7.16565\n",
      "percentile_10      4\n",
      "percentile_25      4\n",
      "percentile_50      4\n",
      "percentile_75      4\n",
      "percentile_90      4\n",
      "-------------  -----------\n"
     ]
    }
   ],
   "source": [
    "# And again for XL data\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT min(lag) as minlag, max(lag) as maxlag, avg(lag) as avglag,\n",
    "        percentile_disc(0.10) within group (order by lag asc) as percentile_10,\n",
    "        percentile_disc(0.25) within group (order by lag asc) as percentile_25,\n",
    "        percentile_disc(0.5) within group (order by lag asc) as percentile_50,\n",
    "        percentile_disc(0.75) within group (order by lag asc) as percentile_75,\n",
    "        percentile_disc(0.90) within group (order by lag asc) as percentile_90\n",
    "        FROM (\n",
    "            SELECT user_id, \n",
    "                   record_time, \n",
    "                   (extract ('epoch' from (record_time - LAG(record_time)\n",
    "                        OVER (\n",
    "                            PARTITION BY (user_id)\n",
    "                            ORDER BY (record_time)\n",
    "                            )))/60)::int \n",
    "                        as lag\n",
    "            FROM level_0.tmpuniqaccelobs\n",
    "            ) lagcalc\n",
    "        WHERE lag > 0\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [[x,row[x]] for x in row.keys()]\n",
    "    print(\"Figure: XL Duty Cycle Intervals (in minutes)\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T20:45:01.288988Z",
     "start_time": "2020-08-10T20:44:18.917305Z"
    },
    "scrolled": false,
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 8123455 GPS lag values into buckets bounded by: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bin_labels(bins, **kwargs):\n",
    "    binw = (max(bins) - min(bins)) / (len(bins)-1)\n",
    "    plt.xticks(np.arange(min(bins)+binw/2, max(bins), binw), bins, **kwargs)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "    \n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    sql = \"\"\"\n",
    "        SELECT lag_minutes \n",
    "        FROM (\n",
    "            SELECT user_id, \n",
    "                   record_time, \n",
    "                   (extract ('epoch' from (record_time - LAG(record_time)\n",
    "                        OVER (\n",
    "                            PARTITION BY (user_id)\n",
    "                            ORDER BY (record_time)\n",
    "                            )))/60.0)\n",
    "                        as lag_minutes\n",
    "            FROM level_0.tmpuniqgpsobs\n",
    "            ) as lagcalc\n",
    "        WHERE lag_minutes > 0 AND lag_minutes < 20\n",
    "        \"\"\"\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    cur.execute(sql)\n",
    "    rows = cur.fetchall()\n",
    "    data = [float(x['lag_minutes']) for x in rows]\n",
    "    bins=list(range(0,21))\n",
    "    print(f\"Binning {len(rows)} GPS lag values\")\n",
    "\n",
    "    plt.hist(data, bins=bins)\n",
    "    bin_labels(bins)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Lag (in minutes)')\n",
    "    plt.ylabel('Number of Observations (log)')\n",
    "    plt.title('Distribution of GPS Observation Lags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-10T21:07:50.517Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# THIS CELL KEEPS CRASHING BEFORE COMPLETE - PROBABLY MEMORY CONSTRAINED\n",
    "# And again for XL\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    sql = \"\"\"\n",
    "        SELECT lag_minutes \n",
    "        FROM (\n",
    "            SELECT user_id, \n",
    "                   record_time, \n",
    "                   (extract ('epoch' from (record_time - LAG(record_time)\n",
    "                        OVER (\n",
    "                            PARTITION BY (user_id)\n",
    "                            ORDER BY (record_time)\n",
    "                            )))/60.0)\n",
    "                        as lag_minutes\n",
    "            FROM level_0.tmpuniqaccelobs\n",
    "            ) as lagcalc\n",
    "        WHERE lag_minutes > 0 AND lag_minutes < 20\n",
    "        \"\"\"\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    cur.execute(sql)\n",
    "    rows = cur.fetchall()\n",
    "    data = [float(x['lag_minutes']) for x in rows]\n",
    "    bins=list(range(0,21))\n",
    "    print(f\"Binning {len(rows)} XL lag values\")\n",
    "\n",
    "    plt.hist(data, bins=bins)\n",
    "    bin_labels(bins)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Lag (in minutes)')\n",
    "    plt.ylabel('Number of Observations (log)')\n",
    "    plt.title('Distribution of XL Observation Lags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did some users gather data for extended periods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T21:44:54.333980Z",
     "start_time": "2020-08-10T21:44:53.658889Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: GPS Sampling Periods\n",
      "--------  -------\n",
      "min_days   0\n",
      "max_days  49\n",
      "avg_days  25.6159\n",
      "--------  -------\n"
     ]
    }
   ],
   "source": [
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT min(dur) min_days, max(dur) max_days, avg(dur) avg_days\n",
    "        FROM (\n",
    "            SELECT num_obs, first, last, extract(days from last-first) as dur \n",
    "            FROM (\n",
    "                SELECT count(1) as num_obs, min(record_time) as first, max(record_time) as last \n",
    "                FROM level_0.tmpuniqgpsobs \n",
    "                GROUP BY user_id\n",
    "             ) count_recs\n",
    "        ) summarize_counts\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [[x,row[x]] for x in row.keys()]\n",
    "    print(\"Figure: GPS Sampling Periods\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T21:46:56.639494Z",
     "start_time": "2020-08-10T21:45:54.706112Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: XL Sampling Periods\n",
      "--------  -------\n",
      "min_days   0\n",
      "max_days  49\n",
      "avg_days  25.7895\n",
      "--------  -------\n"
     ]
    }
   ],
   "source": [
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT min(dur) min_days, max(dur) max_days, avg(dur) avg_days\n",
    "        FROM (\n",
    "            SELECT num_obs, first, last, extract(days from last-first) as dur \n",
    "            FROM (\n",
    "                SELECT count(1) as num_obs, min(record_time) as first, max(record_time) as last \n",
    "                FROM level_0.tmpuniqaccelobs \n",
    "                GROUP BY user_id\n",
    "             ) count_recs\n",
    "        ) summarize_counts\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [[x,row[x]] for x in row.keys()]\n",
    "    print(\"Figure: XL Sampling Periods\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the average collection window for each user is well within the 28-day maximum, extra-wide collection periods do not account for the extra data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did some users contribute unusual numbers of observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:21:26.421035Z",
     "start_time": "2020-08-10T22:20:30.794100Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Summary XL Observations Per User\n",
      "---------------------  -----------\n",
      "expected_obs_per_user    9,676,800\n",
      "  fewest_obs_per_user          968\n",
      "    most_obs_per_user   24,909,419\n",
      "     avg_obs_per_user  5,782,601.2\n",
      "---------------------  -----------\n"
     ]
    }
   ],
   "source": [
    "# Let's compare the average number of observations per user to the expected number\n",
    "study_length_days = 28\n",
    "duty_cycles_per_hour = 12\n",
    "sample_freq = 20\n",
    "samples_per_cycle = 60 * sample_freq\n",
    "expected_obs_per_user = study_length_days * 24 * duty_cycles_per_hour * samples_per_cycle\n",
    "\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT min(num_obs) as fewest_obs, max(num_obs) as most_obs, avg(num_obs) as avg_obs\n",
    "        FROM (\n",
    "            SELECT count(1) num_obs \n",
    "            FROM level_0.tmpuniqaccelobs\n",
    "            GROUP BY user_id\n",
    "        ) count_obs\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['expected_obs_per_user', f\"{expected_obs_per_user:,}\"],\n",
    "            ['fewest_obs_per_user', f\"{row['fewest_obs']:,}\"],\n",
    "            ['most_obs_per_user', f\"{row['most_obs']:,}\"],\n",
    "            ['avg_obs_per_user', f\"{row['avg_obs']:,.1f}\"],\n",
    "           ]\n",
    "    print(\"Figure: Summary XL Observations Per User\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the average is well within expected norms. It may be worth exploring that max case a bit further, but it doesn't account for having 5 times the expected number of records.\n",
    "\n",
    "However, that raises another question. Perhaps it isn't a case of XL having too many records; perhaps it's a case of GPS having too few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:26:30.136100Z",
     "start_time": "2020-08-10T22:26:29.518359Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Summary GPS Observations Per User\n",
      "---------------------  ---------\n",
      "expected_obs_per_user    483,840\n",
      "  fewest_obs_per_user          7\n",
      "    most_obs_per_user  1,018,278\n",
      "     avg_obs_per_user   53,849.5\n",
      "---------------------  ---------\n"
     ]
    }
   ],
   "source": [
    "# Let's compare the average number of observations per user to the expected number for GPS\n",
    "study_length_days = 28\n",
    "duty_cycles_per_hour = 12\n",
    "sample_freq = 1\n",
    "samples_per_cycle = 60 * sample_freq\n",
    "expected_obs_per_user = study_length_days * 24 * duty_cycles_per_hour * samples_per_cycle\n",
    "\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT min(num_obs) as fewest_obs, max(num_obs) as most_obs, avg(num_obs) as avg_obs\n",
    "        FROM (\n",
    "            SELECT count(1) num_obs \n",
    "            FROM level_0.tmpuniqgpsobs\n",
    "            GROUP BY user_id\n",
    "        ) count_obs\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['expected_obs_per_user', f\"{expected_obs_per_user:,}\"],\n",
    "            ['fewest_obs_per_user', f\"{row['fewest_obs']:,}\"],\n",
    "            ['most_obs_per_user', f\"{row['most_obs']:,}\"],\n",
    "            ['avg_obs_per_user', f\"{row['avg_obs']:,.1f}\"],\n",
    "           ]\n",
    "    print(\"Figure: Summary GPS Observations Per User\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we finally have an answer. The number of GPS observations are only about 20% of the theoretical max. This *might* be explained by poor satellite visibility, but it was my understanding that in such situations, low-accuracy samples were still being collected. It will be up to the TDT team to determine whether this is in fact a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "source": [
    "Weird. I fully expected to find no actual duty cycles here, but there they are - 4-minute lags, just as expected. The GPS reports mostly 5-minute lags instead of 4, but I'm not sure why. It's probably worth drawing the histogram to see what's going on.\n",
    "\n",
    "EXCEPT THIS HISTOGRAM WANTS TO RUN FOR MORE THAN A DAY, SO PROBABLY NOT WORTH IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DC Correlations\n",
    "\n",
    "Since the GPS and XL tables do not appear to be synchronized at the observation level, we should now assess whether they are at least synchronized at the DC level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfiltered Duty Cycles\n",
    "We'll start by looking at the sync between the DCs found in the unfiltered observations. (Recall, this means that the duplicates have all been reduced to unique timestamps, but have not been removed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T17:56:31.059075Z",
     "start_time": "2020-08-09T17:52:47.019Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Figure: Comparison of DC Observation Matches:\")\n",
    "table = [[\"Num GPS DCs\",f\"{17:,}\"], \n",
    "         [\"Num Accel DCs\",f\"{19:,}\"], \n",
    "         [\"Exact Obsv Matches\",num_dc_matches], \n",
    "         [\"Match Rate\",f\"{100*float(num_dc_matches)/float(17):0.1f}%\"]]\n",
    "\n",
    "\n",
    "print(tabulate(table,stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T21:32:23.856230Z",
     "start_time": "2020-08-10T21:32:22.894716Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Count of Synchronized DCs in GPS vs XL\n",
      "------------------------  ---------\n",
      "             num_gps_dcs  663625\n",
      "    avg_gps_dcs_per_user    4394.87\n",
      "         num_matched_dcs  659119\n",
      "       num_matched_users     151\n",
      "avg_matched_dcs_per_user    4365.03\n",
      "     match_percentile_10     247\n",
      "     match_percentile_25    1879\n",
      "     match_percentile_50    5026\n",
      "     match_percentile_75    6642\n",
      "     match_percentile_90    7679\n",
      "------------------------  ---------\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "# Count the left join matches for DCs between GPS and Accel tables\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT  sum(count) as num_matched_dcs, \n",
    "                count(1) as num_matched_users, \n",
    "                avg(count) as avg_matched_dcs_per_user,\n",
    "                percentile_disc(0.10) within group (order by count asc) as match_percentile_10,\n",
    "                percentile_disc(0.25) within group (order by count asc) as match_percentile_25,\n",
    "                percentile_disc(0.5) within group (order by count asc) as match_percentile_50,\n",
    "                percentile_disc(0.75) within group (order by count asc) as match_percentile_75,\n",
    "                percentile_disc(0.90) within group (order by count asc) as match_percentile_90\n",
    "        FROM (\n",
    "            SELECT count(1) as count, min(gps.user_id) as user_id, min(gps.dc) as dc\n",
    "            FROM level_0.tmpgpsdcs as gps\n",
    "            INNER JOIN level_0.tmpacceldcs as accel\n",
    "                ON gps.dc = accel.dc \n",
    "                AND gps.user_id = accel.user_id\n",
    "            GROUP BY gps.user_id\n",
    "            ) find_matches\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_gps_dcs', num_gps_dcs],\n",
    "            ['avg_gps_dcs_per_user', avg_gps_dcs_per_user],]\n",
    "    data.extend([[x,row[x]] for x in row.keys()])\n",
    "    print(\"Figure: Count of Synchronized DCs in GPS vs XL\")\n",
    "    print(tabulate(data,stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all seems reasonable, but it does not take into account the viability of the DCs involved, so we'll look at that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T19:43:29.314869Z",
     "start_time": "2020-08-10T19:43:28.936932Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Count of Synchronized Viable DCs in GPS vs XL\n",
      "------------------------  ---------\n",
      "         num_matched_dcs  80428\n",
      "       num_matched_users    145\n",
      "avg_matched_dcs_per_user    554.676\n",
      "     match_percentile_10      9\n",
      "     match_percentile_25     40\n",
      "     match_percentile_50    125\n",
      "     match_percentile_75    811\n",
      "     match_percentile_90   1415\n",
      "------------------------  ---------\n"
     ]
    }
   ],
   "source": [
    "# Count the left join matches for VIABLE DCs between GPS and Accel tables\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT  sum(count) as num_matched_dcs, \n",
    "                count(1) as num_matched_users, \n",
    "                avg(count) as avg_matched_dcs_per_user,\n",
    "                percentile_disc(0.10) within group (order by count asc) as match_percentile_10,\n",
    "                percentile_disc(0.25) within group (order by count asc) as match_percentile_25,\n",
    "                percentile_disc(0.5) within group (order by count asc) as match_percentile_50,\n",
    "                percentile_disc(0.75) within group (order by count asc) as match_percentile_75,\n",
    "                percentile_disc(0.90) within group (order by count asc) as match_percentile_90\n",
    "        FROM (\n",
    "            SELECT count(1) as count, min(gps.user_id) as user_id, min(gps.dc) as dc\n",
    "            FROM level_0.tmpgpsdcs as gps\n",
    "            INNER JOIN level_0.tmpacceldcs as accel\n",
    "                ON gps.dc = accel.dc \n",
    "                AND gps.user_id = accel.user_id\n",
    "            WHERE gps.recs_per_dc >= 10    -- make sure GPS DC is viable\n",
    "            AND   accel.recs_per_dc >= 120 -- make sure XL DC is viable\n",
    "            GROUP BY gps.user_id\n",
    "            ) find_matches\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [[x,row[x]] for x in row.keys()]\n",
    "    print(\"Figure: Count of Synchronized Viable DCs in GPS vs XL\")\n",
    "    print(tabulate(data,stralign='right'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be a concern. When we limit DCs to those with viable observation counts, the number of synchronized pairs drops from 4300 per user to less than 600.\n",
    "\n",
    "I'm not sure if the analysis permits \"hybrid\" pairs (where one of DC or XL is viable, but the other is not), but in case that's an option, we'll count those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T21:37:06.267862Z",
     "start_time": "2020-08-10T21:37:05.414220Z"
    },
    "scrolled": true,
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Count of Synchronized Semi-Viable DCs in GPS vs XL\n",
      "------------------------  ---------\n",
      "           num_accel_dcs  715499\n",
      "  avg_accel_dcs_per_user    4707.23\n",
      "         num_matched_dcs  657930\n",
      "       num_matched_users     151\n",
      "avg_matched_dcs_per_user    4357.15\n",
      "     match_percentile_10     236\n",
      "     match_percentile_25    1878\n",
      "     match_percentile_50    5015\n",
      "     match_percentile_75    6609\n",
      "     match_percentile_90    7675\n",
      "------------------------  ---------\n"
     ]
    }
   ],
   "source": [
    "# Out of curiosity, count the left join matches where at least one of the DCs is viable\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    sql = \"\"\"\n",
    "        SELECT  sum(count) as num_matched_dcs, \n",
    "                count(1) as num_matched_users, \n",
    "                avg(count) as avg_matched_dcs_per_user,\n",
    "                percentile_disc(0.10) within group (order by count asc) as match_percentile_10,\n",
    "                percentile_disc(0.25) within group (order by count asc) as match_percentile_25,\n",
    "                percentile_disc(0.5) within group (order by count asc) as match_percentile_50,\n",
    "                percentile_disc(0.75) within group (order by count asc) as match_percentile_75,\n",
    "                percentile_disc(0.90) within group (order by count asc) as match_percentile_90\n",
    "        FROM (\n",
    "            SELECT count(1) as count, min(gps.user_id) as user_id, min(gps.dc) as dc\n",
    "            FROM level_0.tmpgpsdcs as gps\n",
    "            INNER JOIN level_0.tmpacceldcs as accel\n",
    "                ON gps.dc = accel.dc \n",
    "                AND gps.user_id = accel.user_id\n",
    "            WHERE gps.recs_per_dc >= 10    -- make sure GPS DC is viable\n",
    "            OR   accel.recs_per_dc >= 120 -- make sure XL DC is viable\n",
    "            GROUP BY gps.user_id\n",
    "            ) find_matches\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_accel_dcs', num_accel_dcs],\n",
    "            ['avg_accel_dcs_per_user', avg_accel_dcs_per_user],]\n",
    "\n",
    "    data.extend([[x,row[x]] for x in row.keys()])\n",
    "    print(\"Figure: Count of Synchronized Semi-Viable DCs in GPS vs XL\")\n",
    "    print(tabulate(data,stralign='right'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if researchers need both DCs to be viable in each matched pair, they will have little data to work with, but if the analysis can be done with hybrid pairs, it looks like they'll have virtually the entire DC set to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TL;DR\n",
    "\n",
    "Count the viable DCs:\n",
    "- There are only 20% of the expected number of GPS observations in the table. Instead of the appx 480K samples a fully observed user would have, we have captured only about 53K unique timestamps for each\n",
    "- Of the approx 8,000 DCs we theoretically observed per user, we have data in a little more than half of them, in both the GPS and XL tables\n",
    "- If we consider only viable DCs (>= 120 observations for XL, >= 10 for GPS)\n",
    "  - We lose about 1% of the XL DCs\n",
    "  - But we drop 88% of the GPS DCs, falling to only 560 viable DCs per user\n",
    "  - That number falls to 545 if we filter out conflicted observations\n",
    "  - And it falls to 541 if we filter out ALL duplicate records\n",
    "  \n",
    "When we examine synchronization between GPS and XL observation timestamps (left joins):\n",
    "- Only 4.3% of GPS observations have matching XL observations prior to filtering\n",
    "- And it drops to 3.8% when either type of duplication filtering is applied\n",
    "\n",
    "Finally, I looked at synchronization at the DC level as well.\n",
    "- In the unfiltered data, virtually all GPS DCs have corresponding XL DCs\n",
    "- When we try to match viable DCs, an average user has only 550 GPS DCs with matching XL DCs (the median is 125)\n",
    "- Curiously, if we require only one of the DCs in each pair to be viable, we lose almost none of them\n",
    "- From this I conclude that it is relatively uncommon for both GPS and XL to be viable at the same time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "185.963px",
    "width": "371.194px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "510.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
