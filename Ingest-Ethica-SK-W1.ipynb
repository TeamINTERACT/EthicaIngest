{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T15:22:40.114157Z",
     "start_time": "2020-08-21T15:21:58.373292Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import subprocess as sub\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from tabulate import tabulate\n",
    "\n",
    "# tell Seaborn that we're producing a document and not a slideshow or poster\n",
    "sns.set()\n",
    "sns.set_context('paper')\n",
    "\n",
    "# expects to find connection credentials in local runtime environment\n",
    "db_host=os.environ['SQL_LOCAL_SERVER']\n",
    "db_host_port=int(os.environ['SQL_LOCAL_PORT'])\n",
    "db_user=os.environ['SQL_USER']\n",
    "db_name=os.environ['SQL_DB']\n",
    "db_schema=os.environ['SQL_SCHEMA']\n",
    "csvdir = \"/home/jeffs/projects/def-dfuller/interact/permanent_archive/Saskatoon/Wave1/Ethica/saskatoon_01/raw\"\n",
    "\n",
    "def log(instr):\n",
    "    print(f\"→ {instr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Log for Ethica, Saskatoon, Wave 1\n",
    "\n",
    "After much investigation and discussion, we have decided to ingest the data in its raw form, with no filters applied, and then create a few obvious subtables from that ingested data. The reasoning is that, due to the nature of the data issues, there appears to be no single cleaning filter we can apply that will be appropriate for all downstream analysis. So we'll ingest the raw table, mark it as toxic so nobody uses it by accident, and then create a few sane starting points that people can actually use.\n",
    "\n",
    "\n",
    "If we're going to have multiple tables like this, we'll want to standardize the table naming conventions. Currently, tables are broken into different schemas based on the temporal resolution, but since all of these starting filters will still have raw-level timing, we'll need to name them with that in mind.\n",
    "\n",
    "## Possible Filters\n",
    "There are two fundamental steps to the filters we've applied during the investigation phase: Prefiltration and uniquification. \n",
    "\n",
    "### Prefiltration\n",
    "The incoming raw data is messy and conflicted. Some records are pure singletons, meaning that they are unique on userid, timestamp, and sensor values. Other records are true duplicates, meaning they occur multiple times but have identical userid, time, and sensor values. We call these \"singleon-equivalent,\" since they can be reduced to singletons by simply deleting all but one occurrence. And then there are \"conflicted duplicates,\" for which there are many records with identical userid and timestamp, but that report multiple conflicting sensor readings within the set.\n",
    "\n",
    "The potential causes of the various kinds of duplicates are numerous: there could be hiccups with different versions of the host phone's OS, memory constraint problems on specific user's phones, different user preference settings, differing versions of the Ethica collection software, various types of power and service outages, etc. At this point, we have not yet found any way to attribute causes to the observed effects. And since we don't know the causes, we cannot use metadata as a way to screen the telemetry records. What we *can* do is use the attributes of the duplicates themselves as an indicator of the data's reliability and choose which ones to from the dataset.\n",
    "\n",
    "#### Singletons vs Singleton-Equivalent vs Conflicted\n",
    "It turns out that the vast majority of the duplicates occur in tight bursts, for just short periods of time. And since we don't know why the duplicates are there, the safest course might be that we should simply eliminate all duplicated records. But this would throw out all the singleton-equivalent records as well, so a second scheme was explored in which we keep the singleton and singleton-equivalent records, but filter out the conflicted duplicates.\n",
    "\n",
    "Unfortunately, there are even deeper wrinkles.\n",
    "\n",
    "#### Record Time vs Satellite Time\n",
    "The GPS data contains four different temporal fields. One contains only the year/month and the other is an undocumented internal counter used by the Ethica software. The remaining two (record_time and satellite_time) both encode complete date and time, down to the millisecond. The record_time field is captured from the host phone's clock, and notes the time at which the sensor was read, while the satellite_time field is taken from the GPS signal and represents the time the signal was broadcast from orbit. In theory, these two fields should only deviate by a small number of milliseconds, but in practice, they at times differ wildly. \n",
    "\n",
    "Worse, the duplication of times mentioned above occurs in both fields, but not in any linear, predictable fashion. Sometimes the satellite time is repeated at multiple differing record times, sometimes it's the other way around, and in some cases, they actually agree. So the result of prefiltering the duplicates will change depending on which time field is used as the timestamp of record. For research that needs only GPS data, the satellite_time seems to be the most reliable. But the record_time stamp is the only temporal field available in the accelerometry (XL) table, so any analysis that needs to synchronize the two tables is forced to use record_time as the timestamp of choice.\n",
    "\n",
    "### Uniquification\n",
    "Depending on which method(s) were used to identify and remove the \"unreliable\" records, there can still be duplication present in the dataset, so the next step is to eliminate the duplicates so that only well-ordered, unique observations remain to be fed to the analysis algorithms.\n",
    "\n",
    "It's easy to confuse the prefiltration and uniquification steps, as they tend to use the same fields and similar criteria, but they need to be considered as distinct operations, because their intended functions differ. Prefiltration is a process of identifying and eliminating records that may be associated with dubious/unreliable behavior in the capture system, but it does not necessarily remove all duplicates from the table, which is what uniquification aims to accomplish.\n",
    "\n",
    "To get a better sense of why we keep the two steps distinct, consider the following scenario: Suppose we decide to reject GPS records with duplicated satellite_times but differing lat/lon values, because we believe that *those* are clearly caused by some kind of hiccup in the phone and should be rejected. Doing so would indeed filter out the data we've marked as unreliable, but it would still leave singleton-equivalent duplicates intact, which would be squashed in the uniquification step. \n",
    "\n",
    "And after doing all that, we might then want to link the GPS and XL table by their record_time fields, which would trigger a *third* filtration step, squashing the record_time duplicates.\n",
    "\n",
    "## Naming convention\n",
    "With all the above in mind, we've devised a naming convention to help identify the contents of the various filtered tables. Each filtration step can be summarized by a fairly terse string: \"delconflsat\" would indicate that we **del**eted all **confl**icted records based on **sat**ellite_time, while \"delduplsat\" reports that we **del**eted all **dupl**icates on that field, instead of just the conflicted ones. For all cases other than the raw table, when duplicated records are not deleted, they are instead \"uniquified\" by collapsing them to single records.\n",
    "\n",
    "The first tables created at ingest time will be the raw tables, loaded as-is from the incoming data, complete with duplicates and conflicts. Other tables can then take their names by adding on the tag(s) of the filtration steps applied.\n",
    "\n",
    "So the table called \"ssk-w1-eth-gps-delconflrec\" indicates that the GPS table was filtered to remove all conflicted record_times, while ssk-w1-eth-xls-delduplsat means the XLS table was filtered to remove all duplicates on the satellite_time field.\n",
    "\n",
    "(Note: As an added precaution, the raw tables have the tag \"TOXIC\" appended to remind researchers not to use its content directly for analysis.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Targets\n",
    "\n",
    "The preliminary ingest for Sask Wave 1 will create the following tables in the level_0 schema:\n",
    "\n",
    "  - ssk-w1-eth-gps-raw-TOXIC\n",
    "  - ssk-w1-eth-xls-raw-TOXIC\n",
    "  - ssk-w1-eth-xls-delduplrec (raw xls with all rec-time duplicates removed)\n",
    "  - ssk-w1-eth-xls-delconflrec (raw xls with all rec-time conflicts removed)\n",
    "  - ssk-w1-eth-gps-delduplsat (raw gps with all sat-time duplicates removed)\n",
    "  - ssk-w1-eth-gps-delduplrec (raw gps with all rec-time duplicates removed)\n",
    "  - ssk-w1-eth-gps-delconflsat (raw gps with all sat-time conflicts removed)\n",
    "  - ssk-w1-eth-gps-delconflrec (raw gps with all rec-time conflicts removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssk_w1_eth_gps_raw_TOXIC\n",
    "\n",
    "Ideally, we want to use the psql COPY command, since it has been\n",
    "optimized for fast loading, but the target telemetry table and the\n",
    "incoming CSV file have different column names. We *COULD* just rename \n",
    "the columns in the CSV file, but that's a manual step that shouldn't\n",
    "be embedded into the process.\n",
    "\n",
    "So instead, we'll load the data into a temporary table, and then transfer it from there into the target table and match the records to their interact_ids at the same time. This method will more or less double the ingest time, but having a reliable ingest process that can be fully documented, without manual interventions, seems like the more robust path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T16:41:50.154350Z",
     "start_time": "2020-08-20T16:41:50.084980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the temporary incoming table and load the raw GPS data into it\n",
    "def load_raw_CSV_data(csvfilepath, schema, tmptablename, tmpsqlvars, desttablename, destsqlvars, transfersql):\n",
    "    log(f\"Loading file '{csvfilepath}' into table '{tmptablename}' of schema '{schema}'\")\n",
    "    with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "        cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "\n",
    "        # Create the temporary onboarding table\n",
    "        fulltmptablename = f\"{schema}.{tmptablename}\"\n",
    "        sql = f\"\"\"\n",
    "            DROP TABLE IF EXISTS {fulltmptablename}; \n",
    "            CREATE TABLE {fulltmptablename} (\n",
    "                {tmpsqlvars}\n",
    "                );\n",
    "                \"\"\"\n",
    "        log(f\"Creating temp ingest table {tmptablename}\")\n",
    "        cur.execute(sql)\n",
    "        # now add a comment describing table's purpose\n",
    "        cur.execute(f\"COMMENT ON TABLE {fulltmptablename} IS 'Temporary table for raw data ingest.';\")\n",
    "\n",
    "        # And load the data from the CSV\n",
    "        log(f\"Ingesting raw CSV\")\n",
    "        with open(csvfilepath, 'r') as f:\n",
    "            # Notice that we don't need the `csv` module.\n",
    "            next(f) # Skip the header row because copy_from doesn't want it\n",
    "            cur.copy_from(f, fulltmptablename, sep=',')\n",
    "\n",
    "        # Now collect some basic stats and validate the loaded raw data\n",
    "        log(f\"Checking success of raw data load\")\n",
    "        rowcount = 0\n",
    "        with open(csvfilepath,'r') as fh:\n",
    "            for line in fh:\n",
    "                rowcount += 1\n",
    "        log(f\"Expecting {rowcount-1:,} lines in table\") # don't count the header      \n",
    "        sql = f\"\"\"SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "                  FROM (\n",
    "                         SELECT COUNT(1) as num_recs\n",
    "                         FROM {fulltmptablename}\n",
    "                         GROUP BY user_id\n",
    "                       ) as records_per_user\n",
    "                  \"\"\"\n",
    "        cur.execute(sql)\n",
    "        row = cur.fetchone()\n",
    "        num_raw_recs = row['num_recs']\n",
    "        num_raw_users = row['num_users']\n",
    "        log(f\"Ingested {num_raw_recs:,} records across {num_raw_users:,} users.\")\n",
    "        if num_raw_recs == rowcount - 1:\n",
    "            log(\"Raw data ingest appears successful.\")\n",
    "            conn.commit()\n",
    "        else:\n",
    "            log(\"ERROR: INGESTED ROW COUNT DOES NOT MATCH SOURCE FILE\")\n",
    "            return\n",
    "              \n",
    "        # Create the final destination table\n",
    "        fulldesttablename = f\"{schema}.{desttablename}\"\n",
    "        sql = f\"\"\"\n",
    "            DROP TABLE IF EXISTS {fulldesttablename} CASCADE; \n",
    "            CREATE TABLE {fulldesttablename} (\n",
    "                {destsqlvars}\n",
    "                )\n",
    "                \"\"\"\n",
    "        log(f\"Creating destination table {fulldesttablename}\")\n",
    "        cur.execute(sql)\n",
    "\n",
    "        # And transfer the data from the staging table\n",
    "        log(f\"Matching ethica_ids to interact_ids and transfering records into destination table\")\n",
    "        cur.execute(transfer_sql)\n",
    "\n",
    "        # And finally collect some basic stats and validate the ingested data\n",
    "        log(f\"Checking success of ingest\")\n",
    "        sql = f\"\"\"SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "                  FROM (\n",
    "                         SELECT COUNT(1) as num_recs\n",
    "                         FROM {fulldesttablename}\n",
    "                         GROUP BY iid\n",
    "                       ) as records_per_user\n",
    "                  \"\"\"\n",
    "        cur.execute(sql)\n",
    "        row = cur.fetchone()\n",
    "        num_final_recs = row['num_recs']\n",
    "        num_final_users = row['num_users']\n",
    "        \n",
    "        # count the number of records for which there are no matching interact_ids\n",
    "        missing_sql = f\"\"\"\n",
    "            SELECT count(1) as num_missing_users, sum(num_recs) as total_missing_recs, min(interact_id)\n",
    "            FROM (\n",
    "                SELECT count(1) as num_recs, min(interact_id) as interact_id, min(user_id), min(ethica_email)\n",
    "                FROM \n",
    "                   portal_dev.ethica_assignments asgn\n",
    "                   RIGHT OUTER JOIN {fulltmptablename} raw\n",
    "                   ON asgn.ethica_id = raw.user_id\n",
    "                GROUP BY user_id\n",
    "            ) recs_per_unmatched_user\n",
    "            WHERE interact_id IS NULL;\n",
    "            \"\"\"\n",
    "        cur.execute(missing_sql)\n",
    "        row = cur.fetchone()\n",
    "        num_missing_recs = int(row['total_missing_recs'] or 0) #assign 0 if no records match\n",
    "        num_missing_users = int(row['num_missing_users'] or 0) #assign 0 if no records match\n",
    "        log(f\"Identified {num_missing_recs:,} records across {num_missing_users:,} users for whom interact_id is not known.\")\n",
    "        \n",
    "        # Report the succes, partial success, or failure of the ingest\n",
    "        log(f\"Transfered {num_final_recs:,} records for {num_final_users:,} users.\")\n",
    "        if num_raw_recs == num_final_recs:\n",
    "            log(\"Data transfer and ingest operation complete. All records ingested.\")\n",
    "            conn.commit()\n",
    "        elif num_raw_recs == num_final_recs + num_missing_recs:\n",
    "            log(f\"Data transfer and ingest operation complete. All records accounted for, but {num_missing_users:,} unknown users.\")\n",
    "            conn.commit()\n",
    "        else:\n",
    "            log(\"ERROR: TRANSFERED ROW COUNT DOES NOT MATCH STAGING TABLE\")\n",
    "            \n",
    "        data = [ ['num_recs', num_final_recs], ['num_users', num_final_users], ]\n",
    "        print(f\"\\nFigure: Counts for {fulldesttablename}\")\n",
    "        print(tabulate(data, floatfmt=',.0f', stralign='right' ))\n",
    "        \n",
    "        \n",
    "        # If there were some unmatched user_ids, list them so operator can investigate\n",
    "        if num_missing_recs:\n",
    "            problems_sql = f\"\"\"\n",
    "                SELECT count(1) as num_recs, \n",
    "                       min(user_id) as user_id\n",
    "                FROM portal_dev.ethica_assignments asgn\n",
    "                   RIGHT OUTER JOIN {fulltmptablename} raw\n",
    "                   ON asgn.ethica_id = raw.user_id\n",
    "                WHERE interact_id IS NULL\n",
    "                GROUP BY user_id;\n",
    "                \"\"\"\n",
    "            print(f\"\\nFigure: Unmatched user_ids from {tmptablename}\")\n",
    "            cur.execute(problems_sql)\n",
    "            rows = cur.fetchall()\n",
    "            print(tabulate(rows, headers='keys'))\n",
    "            print(f\"Total Unmatched Rows: {num_missing_recs:,}\")\n",
    "#        else:\n",
    "#            log(\"Record count in CSV matches record count of ingested table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T16:44:45.712239Z",
     "start_time": "2020-08-20T16:41:50.158960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Loading file '/home/jeffs/projects/def-dfuller/interact/permanent_archive/Saskatoon/Wave1/Ethica/saskatoon_01/raw/gps.csv' into table 'tmpgps' of schema 'level_0'\n",
      "→ Creating temp ingest table tmpgps\n",
      "→ Ingesting raw CSV\n",
      "→ Checking success of raw data load\n",
      "→ Expecting 28,897,052 lines in table\n",
      "→ Ingested 28,897,052 records across 151 users.\n",
      "→ Raw data ingest appears successful.\n",
      "→ Creating destination table level_0.ssk_w1_eth_gps_raw_TOXIC\n",
      "→ Matching ethica_ids to interact_ids and transfering records into destination table\n",
      "→ Checking success of ingest\n",
      "→ Identified 0 records across 0 users for whom interact_id is not known.\n",
      "→ Transfered 28,897,052 records for 151 users.\n",
      "→ Data transfer and ingest operation complete. All records ingested.\n",
      "\n",
      "Figure: Counts for level_0.ssk_w1_eth_gps_raw_TOXIC\n",
      "---------  ----------\n",
      " num_recs  28,897,052\n",
      "num_users         151\n",
      "---------  ----------\n"
     ]
    }
   ],
   "source": [
    "# Set up the parameters for loading the GPS table data\n",
    "csvfilename = \"gps.csv\"\n",
    "fullcsvpath = os.path.join(csvdir, csvfilename)\n",
    "tmptablename = \"tmpgps\"\n",
    "desttablename = \"ssk_w1_eth_gps_raw_TOXIC\"\n",
    "\n",
    "tmpsqlvars = \"\"\"\n",
    "    user_id BIGINT NOT NULL,\n",
    "    date TEXT,\n",
    "    device_id TEXT NOT NULL,\n",
    "    record_time TIMESTAMP WITH TIME ZONE NOT NULL,\n",
    "    timestamp TEXT,\n",
    "    accu DOUBLE PRECISION,\n",
    "    alt DOUBLE PRECISION,\n",
    "    bearing DOUBLE PRECISION,\n",
    "    lat DOUBLE PRECISION NOT NULL,\n",
    "    lon DOUBLE PRECISION NOT NULL,\n",
    "    provider TEXT,\n",
    "    satellite_time TIMESTAMP WITH TIME ZONE,\n",
    "    speed DOUBLE PRECISION\n",
    "    \"\"\"\n",
    "\n",
    "destsqlvars = \"\"\"\n",
    "    iid BIGINT NOT NULL,  -- interact_id\n",
    "    record_time TIMESTAMP WITH TIME ZONE NOT NULL, -- participant's UTC time, to millisec, from phone clock\n",
    "    satellite_time TIMESTAMP WITH TIME ZONE NOT NULL, -- timestamp taken from satellite data\n",
    "    lat DOUBLE PRECISION NOT NULL,\n",
    "    lon DOUBLE PRECISION NOT NULL,\n",
    "    speed DOUBLE PRECISION DEFAULT 'NaN',\n",
    "    course DOUBLE PRECISION DEFAULT 'NaN',\n",
    "    alt DOUBLE PRECISION DEFAULT 'NaN',\n",
    "    accu DOUBLE PRECISION DEFAULT 'NaN',\n",
    "    provider TEXT DEFAULT ''\n",
    "    \"\"\"\n",
    "\n",
    "transfer_sql = f\"\"\"\n",
    "   INSERT INTO {db_schema}.{desttablename} (iid,record_time,satellite_time,lat,lon,speed,course,alt,accu,provider)\n",
    "        SELECT asgn.interact_id,\n",
    "                raw.record_time,\n",
    "                raw.satellite_time,\n",
    "                raw.lat,\n",
    "                raw.lon,\n",
    "                raw.speed,\n",
    "                raw.bearing,\n",
    "                raw.alt,\n",
    "                raw.accu,\n",
    "                raw.provider\n",
    "        FROM {db_schema}.{tmptablename} raw \n",
    "            INNER JOIN portal_dev.ethica_assignments AS asgn\n",
    "            ON raw.user_id = asgn.ethica_id\n",
    "        \"\"\"\n",
    "load_raw_CSV_data(fullcsvpath, db_schema, tmptablename, tmpsqlvars, desttablename, destsqlvars, transfer_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T16:44:48.450471Z",
     "start_time": "2020-08-20T16:44:45.787186Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Validating table 'level_0.ssk_w1_eth_gps_raw_TOXIC'\n",
      "\n",
      "---------  ----------\n",
      " num_recs  28,897,052\n",
      "num_users         151\n",
      "---------  ----------\n"
     ]
    }
   ],
   "source": [
    "# Quick block that can be run at any time to validate the existing table\n",
    "desttablename = \"ssk_w1_eth_gps_raw_TOXIC\"\n",
    "fulldesttablename = f\"{db_schema}.{desttablename}\"\n",
    "print(f\"Figure: Validating table '{fulldesttablename}'\\n\")\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    \n",
    "\n",
    "    # Collect the same basic stats as computed during ingest\n",
    "    sql = f\"\"\"SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "              FROM (\n",
    "                     SELECT COUNT(1) as num_recs\n",
    "                     FROM {fulldesttablename}\n",
    "                     GROUP BY iid\n",
    "                   ) as records_per_user\n",
    "              \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "\n",
    "\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T16:44:52.772268Z",
     "start_time": "2020-08-20T16:44:48.453692Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# This is the code that is needed to add a simple checksum to the ingest report \n",
    "# but I decided it was overkill.\n",
    "import hashlib\n",
    "\n",
    "hasher = f\"\"\"\n",
    "    SELECT SUM(iid\n",
    "              + extract('epoch' from record_time)                \n",
    "              - extract('epoch' from satellite_time)\n",
    "              + lat\n",
    "              - lon\n",
    "              + speed\n",
    "              - course\n",
    "              + alt\n",
    "              - accu \n",
    "              )::bigint as simple_total\n",
    "    FROM {fulldesttablename} ;\n",
    "    \"\"\"\n",
    "\n",
    "cur.execute(hasher)\n",
    "row = cur.fetchone()\n",
    "fingerprint = hashlib.md5(row['simple_total'].to_bytes(8, 'big', signed=True)).hexdigest()\n",
    "data.append(['simple_checksum',fingerprint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssk_w1_eth_xls_raw_TOXIC\n",
    "Proceeds as per the GPS table, but with accelerometry source file and column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T18:04:43.825460Z",
     "start_time": "2020-08-20T16:44:52.775441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Loading file '/home/jeffs/projects/def-dfuller/interact/permanent_archive/Saskatoon/Wave1/Ethica/saskatoon_01/raw/accelerometer.csv' into table 'tmpxl' of schema 'level_0'\n",
      "→ Creating temp ingest table tmpxl\n",
      "→ Ingesting raw CSV\n",
      "→ Checking success of raw data load\n",
      "→ Expecting 960,178,307 lines in table\n",
      "→ Ingested 960,178,307 records across 152 users.\n",
      "→ Raw data ingest appears successful.\n",
      "→ Creating destination table level_0.ssk_w1_eth_xls_raw_TOXIC\n",
      "→ Matching ethica_ids to interact_ids and transfering records into destination table\n",
      "→ Checking success of ingest\n",
      "→ Identified 0 records across 0 users for whom interact_id is not known.\n",
      "→ Transfered 960,178,307 records for 152 users.\n",
      "→ Data transfer and ingest operation complete. All records ingested.\n",
      "\n",
      "Figure: Counts for level_0.ssk_w1_eth_xls_raw_TOXIC\n",
      "---------  -----------\n",
      " num_recs  960,178,307\n",
      "num_users          152\n",
      "---------  -----------\n"
     ]
    }
   ],
   "source": [
    "# Set up the parameters for loading the XL table data\n",
    "csvfilename = \"accelerometer.csv\"\n",
    "fullcsvpath = os.path.join(csvdir, csvfilename)\n",
    "tmptablename = \"tmpxl\"\n",
    "desttablename = \"ssk_w1_eth_xls_raw_TOXIC\"\n",
    "\n",
    "tmpsqlvars = \"\"\"\n",
    "    user_id BIGINT NOT NULL,\n",
    "    date TEXT,\n",
    "    device_id TEXT NOT NULL,\n",
    "    record_time TIMESTAMP WITH TIME ZONE NOT NULL,\n",
    "    timestamp TEXT,\n",
    "    accu DOUBLE PRECISION,\n",
    "    x_axis DOUBLE PRECISION NOT NULL,\n",
    "    y_axis DOUBLE PRECISION NOT NULL,\n",
    "    z_axis DOUBLE PRECISION NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "destsqlvars = \"\"\"\n",
    "    iid BIGINT NOT NULL,   -- interact_id\n",
    "    record_time TIMESTAMP WITH TIME ZONE NOT NULL, -- participant's UTC time, to millisec \n",
    "    x DOUBLE PRECISION NOT NULL,\n",
    "    y DOUBLE PRECISION NOT NULL,\n",
    "    z DOUBLE PRECISION NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "transfer_sql = f\"\"\"\n",
    "    INSERT INTO {db_schema}.{desttablename} (iid,record_time,x,y,z)\n",
    "    SELECT asgn.interact_id,\n",
    "        raw.record_time,\n",
    "        raw.x_axis,\n",
    "        raw.y_axis,\n",
    "        raw.z_axis\n",
    "    FROM {db_schema}.{tmptablename} raw \n",
    "        INNER JOIN portal_dev.ethica_assignments asgn\n",
    "        ON raw.user_id = asgn.ethica_id \n",
    "        \"\"\"\n",
    "load_raw_CSV_data(fullcsvpath, db_schema, tmptablename, tmpsqlvars, desttablename, destsqlvars, transfer_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T18:06:05.279914Z",
     "start_time": "2020-08-20T18:04:43.879573Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure: Validating table 'level_0.ssk_w1_eth_xls_raw_TOXIC'\n",
      "\n",
      "---------  -----------\n",
      " num_recs  960,178,307\n",
      "num_users          152\n",
      "---------  -----------\n"
     ]
    }
   ],
   "source": [
    "# Quick block that can be run at any time to validate the existing XL table\n",
    "desttablename = \"ssk_w1_eth_xls_raw_TOXIC\"\n",
    "fulldesttablename = f\"{db_schema}.{desttablename}\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    \n",
    "    # Collect the same basic stats as computed during ingest\n",
    "    sql = f\"\"\"SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "              FROM (\n",
    "                     SELECT COUNT(1) as num_recs\n",
    "                     FROM {fulldesttablename}\n",
    "                     GROUP BY iid\n",
    "                   ) as records_per_user\n",
    "              \"\"\"\n",
    "    print(f\"Figure: Validating table '{fulldesttablename}'\\n\")\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing population of both tables\n",
    "As a final validation, verify that the GPS and XLS raw tables both have data from the same users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T18:08:52.769828Z",
     "start_time": "2020-08-20T18:06:05.283097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Figure: Users with data present in only one table\n",
      "\n",
      "  gps_iid    xls_iid\n",
      "---------  ---------\n",
      "           302562672\n"
     ]
    }
   ],
   "source": [
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "   \n",
    "    mismatch_sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP TABLE IF EXISTS tmpgpsiids;\n",
    "        DROP TABLE IF EXISTS tmpxlsiids;\n",
    "        CREATE TABLE tmpgpsiids AS SELECT DISTINCT iid FROM ssk_w1_eth_gps_raw_toxic ;\n",
    "        CREATE TABLE tmpxlsiids AS SELECT DISTINCT iid FROM ssk_w1_eth_xls_raw_toxic ;\n",
    "        SELECT tmpgpsiids.iid AS gps_iid, tmpxlsiids.iid AS xls_iid \n",
    "        FROM tmpgpsiids \n",
    "             RIGHT OUTER JOIN tmpxlsiids \n",
    "             ON tmpgpsiids.iid = tmpxlsiids.iid \n",
    "        WHERE tmpgpsiids.iid IS NULL \n",
    "           OR tmpxlsiids.iid IS NULL;\n",
    "        \"\"\"\n",
    "    \n",
    "    print(f\"\\nFigure: Users with data present in only one table\\n\")\n",
    "    cur.execute(mismatch_sql)\n",
    "    rows = cur.fetchall()\n",
    "    print(tabulate(rows, headers='keys', stralign='right'))\n",
    "\n",
    "    # Interact_id 302562672 is known to have no GPS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssk-w1-eth-xls-delduplrec \n",
    "This materialized view will be extracted from the raw XLS data, with all record_time **duplicates** removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T20:15:11.901818Z",
     "start_time": "2020-08-20T19:34:47.052304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Creating ssk_w1_eth_xls_delduplrec as a materialized view...\n",
      "→ Done\n",
      "\n",
      "Figure: Basic counts for table ssk_w1_eth_xls_delduplrec\n",
      "\n",
      "---------  -----------\n",
      " num_recs  832,152,864\n",
      "num_users          152\n",
      "---------  -----------\n"
     ]
    }
   ],
   "source": [
    "tablename = 'ssk_w1_eth_xls_delduplrec'\n",
    "sourcetable = \"ssk_w1_eth_xls_raw_TOXIC\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    log(f\"Creating {tablename} as a materialized view...\")\n",
    "    sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP MATERIALIZED VIEW IF EXISTS {tablename};\n",
    "        CREATE MATERIALIZED VIEW {tablename}\n",
    "        AS\n",
    "            SELECT iid, record_time, x, y, z\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       iid, \n",
    "                       record_time,\n",
    "                       min(x) as x,\n",
    "                       min(y) as y,\n",
    "                       min(z) as z\n",
    "                FROM {sourcetable}\n",
    "                GROUP BY iid, record_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1; -- there are no duplicates at all\n",
    "            CREATE UNIQUE INDEX {tablename}_idx ON {tablename} (iid, record_time);\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    log(f\"Done\")\n",
    "    \n",
    "    # Report basic stats on the newly created view\n",
    "    print(f\"\\nFigure: Basic counts for table {tablename}\\n\")\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssk-w1-eth-gps-delduplrec \n",
    "This materialized view will be extracted from the raw GPS data, with all record_time **duplicates** removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T19:34:13.779268Z",
     "start_time": "2020-08-20T19:33:35.085467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Creating ssk_w1_eth_gps_delduplrec as a materialized view...\n",
      "→ Done\n",
      "\n",
      "Figure: Basic counts for table ssk_w1_eth_gps_delduplrec\n",
      "\n",
      "---------  ---------\n",
      " num_recs  7,729,649\n",
      "num_users        151\n",
      "---------  ---------\n"
     ]
    }
   ],
   "source": [
    "tablename = 'ssk_w1_eth_gps_delduplrec'\n",
    "sourcetable = \"ssk_w1_eth_gps_raw_TOXIC\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    log(f\"Creating {tablename} as a materialized view...\")\n",
    "    sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP MATERIALIZED VIEW IF EXISTS {tablename};\n",
    "        CREATE MATERIALIZED VIEW {tablename}\n",
    "        AS\n",
    "            SELECT iid, record_time, satellite_time, minlat as lat, minlon as lon\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       iid, \n",
    "                       record_time, \n",
    "                       min(satellite_time) as satellite_time,\n",
    "                       min(lat) as minlat, \n",
    "                       max(lat) as maxlat,\n",
    "                       min(lon) as minlon,\n",
    "                       max(lon) as maxlon\n",
    "                FROM {sourcetable}\n",
    "                GROUP BY iid, record_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1; -- there are no duplicates at all\n",
    "            CREATE UNIQUE INDEX {tablename}_idx ON {tablename} (iid, record_time);\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    log(f\"Done\")\n",
    "    \n",
    "    # Report basic stats on the newly created view\n",
    "    print(f\"\\nFigure: Basic counts for table {tablename}\\n\")\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssk-w1-eth-gps-delconflrec \n",
    "This materialized view will be extracted from the raw GPS data, with all record_time **conflicts** removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T03:16:01.346908Z",
     "start_time": "2020-08-21T03:15:18.594975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Creating ssk_w1_eth_gps_delconflrec as a materialized view...\n",
      "→ Done\n",
      "\n",
      "Figure: Basic counts for table ssk_w1_eth_gps_delconflrec\n",
      "\n",
      "---------  ---------\n",
      " num_recs  7,791,943\n",
      "num_users        151\n",
      "---------  ---------\n"
     ]
    }
   ],
   "source": [
    "tablename = 'ssk_w1_eth_gps_delconflrec'\n",
    "sourcetable = \"ssk_w1_eth_gps_raw_TOXIC\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    log(f\"Creating {tablename} as a materialized view...\")\n",
    "    sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP MATERIALIZED VIEW IF EXISTS {tablename};\n",
    "        CREATE MATERIALIZED VIEW {tablename}\n",
    "        AS\n",
    "            SELECT iid, record_time, satellite_time, minlat as lat, minlon as lon\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       iid, \n",
    "                       record_time, \n",
    "                       min(satellite_time) as satellite_time,\n",
    "                       min(lat) as minlat, \n",
    "                       max(lat) as maxlat,\n",
    "                       min(lon) as minlon,\n",
    "                       max(lon) as maxlon\n",
    "                FROM {sourcetable}\n",
    "                GROUP BY iid, record_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1 -- there are no duplicates at all\n",
    "               OR (minlon = maxlon AND minlat = maxlat); -- the duplicates are identical\n",
    "        CREATE UNIQUE INDEX {tablename}_idx ON {tablename} (iid, record_time);\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    log(f\"Done\")\n",
    "    \n",
    "    # Report basic stats on the newly created view\n",
    "    print(f\"\\nFigure: Basic counts for table {tablename}\\n\")\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssk-w1-eth-xls-delconflrec \n",
    "This materialized view will be extracted from the raw XL data, with all record_time **conflicts** removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T16:04:27.485222Z",
     "start_time": "2020-08-21T15:22:54.732800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Creating ssk_w1_eth_xls_delconflrec as a materialized view...\n",
      "→ Done\n",
      "\n",
      "Figure: Basic counts for table ssk_w1_eth_xls_delconflrec\n",
      "\n",
      "---------  -----------\n",
      " num_recs  836,034,501\n",
      "num_users          152\n",
      "---------  -----------\n"
     ]
    }
   ],
   "source": [
    "tablename = 'ssk_w1_eth_xls_delconflrec'\n",
    "sourcetable = \"ssk_w1_eth_xls_raw_TOXIC\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    log(f\"Creating {tablename} as a materialized view...\")\n",
    "    sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP MATERIALIZED VIEW IF EXISTS {tablename};\n",
    "        CREATE MATERIALIZED VIEW {tablename}\n",
    "        AS\n",
    "            SELECT iid, record_time, minx as x, miny as y, minz as z\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       iid, \n",
    "                       record_time,\n",
    "                       min(x) as minx,\n",
    "                       max(x) as maxx,\n",
    "                       min(y) as miny,\n",
    "                       max(y) as maxy,\n",
    "                       min(z) as minz,\n",
    "                       max(z) as maxz\n",
    "                FROM {sourcetable}\n",
    "                GROUP BY iid, record_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1 -- there are no duplicates at all\n",
    "               OR (minx = maxx AND miny = maxy AND minz = maxz); -- the duplicates are identical\n",
    "            CREATE UNIQUE INDEX {tablename}_idx ON {tablename} (iid, record_time);\n",
    "            \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    log(f\"Done\")\n",
    "    \n",
    "    # Report basic stats on the newly created view\n",
    "    print(f\"\\nFigure: Basic counts for table {tablename}\\n\")\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssk-w1-eth-gps-delduplsat\n",
    "This materialized view will be extracted from the raw GPS data, with all satellite_time **duplicates** removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T16:05:00.166998Z",
     "start_time": "2020-08-21T16:04:27.556891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Creating ssk_w1_eth_gps_delduplsat as a materialized view...\n",
      "→ Done\n",
      "\n",
      "Figure: Basic counts for table ssk_w1_eth_gps_delduplsat\n",
      "\n",
      "---------  ---------\n",
      " num_recs  6,262,936\n",
      "num_users        151\n",
      "---------  ---------\n"
     ]
    }
   ],
   "source": [
    "tablename = 'ssk_w1_eth_gps_delduplsat'\n",
    "sourcetable = \"ssk_w1_eth_gps_raw_TOXIC\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    log(f\"Creating {tablename} as a materialized view...\")\n",
    "    sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP MATERIALIZED VIEW IF EXISTS {tablename};\n",
    "        CREATE MATERIALIZED VIEW {tablename}\n",
    "        AS\n",
    "            SELECT iid, record_time, satellite_time, minlat as lat, minlon as lon\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       iid, \n",
    "                       min(record_time) as record_time, \n",
    "                       satellite_time,\n",
    "                       min(lat) as minlat, \n",
    "                       max(lat) as maxlat,\n",
    "                       min(lon) as minlon,\n",
    "                       max(lon) as maxlon\n",
    "                FROM {sourcetable}\n",
    "                GROUP BY iid, satellite_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1; -- there are no duplicates at all\n",
    "        CREATE UNIQUE INDEX {tablename}_idx ON {tablename} (iid, satellite_time);\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    log(f\"Done\")\n",
    "    \n",
    "    # Report basic stats on the newly created view\n",
    "    print(f\"\\nFigure: Basic counts for table {tablename}\\n\")\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssk-w1-eth-gps-delconflsat\n",
    "This materialized view will be extracted from the raw GPS data, with all satellite_time **conflicts** removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T16:05:35.048148Z",
     "start_time": "2020-08-21T16:05:00.170553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Creating ssk_w1_eth_gps_delconflsat as a materialized view...\n",
      "→ Done\n",
      "\n",
      "Figure: Basic counts for table ssk_w1_eth_gps_delconflsat\n",
      "\n",
      "---------  ---------\n",
      " num_recs  7,275,243\n",
      "num_users        151\n",
      "---------  ---------\n"
     ]
    }
   ],
   "source": [
    "tablename = 'ssk_w1_eth_gps_delconflsat'\n",
    "sourcetable = \"ssk_w1_eth_gps_raw_TOXIC\"\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    log(f\"Creating {tablename} as a materialized view...\")\n",
    "    sql = f\"\"\"\n",
    "        SET SCHEMA '{db_schema}';\n",
    "        DROP MATERIALIZED VIEW IF EXISTS {tablename};\n",
    "        CREATE MATERIALIZED VIEW {tablename}\n",
    "        AS\n",
    "            SELECT iid, record_time, satellite_time, minlat as lat, minlon as lon\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       iid, \n",
    "                       min(record_time) as record_time, \n",
    "                       satellite_time,\n",
    "                       min(lat) as minlat, \n",
    "                       max(lat) as maxlat,\n",
    "                       min(lon) as minlon,\n",
    "                       max(lon) as maxlon\n",
    "                FROM {sourcetable}\n",
    "                GROUP BY iid, satellite_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1 -- there are no duplicates at all\n",
    "               OR (minlon = maxlon AND minlat = maxlat); -- the duplicates are identical\n",
    "        CREATE UNIQUE INDEX {tablename}_idx ON {tablename} (iid, satellite_time);\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    log(f\"Done\")\n",
    "    \n",
    "    # Report basic stats on the newly created view\n",
    "    print(f\"\\nFigure: Basic counts for table {tablename}\\n\")\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T18:08:52.978905Z",
     "start_time": "2020-08-20T16:41:49.516Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "tablename = 'ssk-w1-eth-xls-delduplrec'\n",
    "# Quick block that can be run at any time to validate counts on any of the above-created tables\n",
    "with psycopg2.connect(user=db_user, host=db_host, port=db_host_port, database='interact_db') as conn:\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "    \n",
    "    # Collect the same basic stats as computed during ingest\n",
    "    sql = f\"\"\"\n",
    "    SET SCHEMA '{db_schema}';\n",
    "    SELECT SUM(num_recs) as num_recs, COUNT(1) as num_users\n",
    "        FROM (\n",
    "             SELECT COUNT(1) as num_recs\n",
    "             FROM {tablename}\n",
    "             GROUP BY iid\n",
    "           ) as records_per_user\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchone()\n",
    "    data = [['num_recs',row['num_recs']], ['num_users',row['num_users']],]\n",
    "    print(f\"Figure: Basic counts for table {tablename})\n",
    "    print(tabulate(data, floatfmt=',.0f', stralign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T18:08:52.980396Z",
     "start_time": "2020-08-20T16:41:49.520Z"
    },
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a table with no conflicted duplicates, and all singleton-equivalent records collapsed into singletons\n",
    "CREATE TABLE IF NOT EXISTS level_0.tmpuniqgpsunconflictedobs AS\n",
    "            SELECT 1 as numrecs, user_id, record_time, minlat as lat, minlon as lon\n",
    "            FROM (\n",
    "                SELECT count(1) as numrows, \n",
    "                       user_id, \n",
    "                       record_time, \n",
    "                       min(lat) as minlat, \n",
    "                       max(lat) as maxlat,\n",
    "                       min(lon) as minlon,\n",
    "                       max(lon) as maxlon\n",
    "                FROM level_0.tmpsskgps\n",
    "                GROUP BY user_id, record_time\n",
    "                ) as count_collisions\n",
    "            WHERE numrows = 1 -- there are no duplicates at all\n",
    "                OR (minlat = maxlat AND minlon = maxlon) -- the duplicates are identical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here endeth the ingest."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
